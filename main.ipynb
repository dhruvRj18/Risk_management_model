{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d11d0575-7192-42a3-ad37-17bca821b8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "011e74cc-653b-4b3e-9ce6-119e7a9b9905",
   "metadata": {},
   "outputs": [],
   "source": [
    "GDP = pd.read_csv(\"data/economic_indicators/GDP\")\n",
    "FEDFUNDS = pd.read_csv(\"data/economic_indicators/FEDFUNDS\")\n",
    "CPIAUCSL = pd.read_csv(\"data/economic_indicators/CPIAUCSL\")\n",
    "UNRATE = pd.read_csv(\"data/economic_indicators/UNRATE\")\n",
    "GS10 = pd.read_csv(\"data/economic_indicators/GS10\")\n",
    "INDPRO = pd.read_csv(\"data/economic_indicators/INDPRO\")\n",
    "PPIACO = pd.read_csv(\"data/economic_indicators/PPIACO\")\n",
    "RSXFS = pd.read_csv(\"data/economic_indicators/RSXFS\")\n",
    "HOUST = pd.read_csv(\"data/economic_indicators/HOUST\")\n",
    "PSAVERT = pd.read_csv(\"data/economic_indicators/PSAVERT\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af8b8dfb-0fc2-43a0-a879-5bad2203a7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(309, 2)\n",
      "(928, 2)\n",
      "(916, 2)\n",
      "(838, 2)\n",
      "(853, 2)\n",
      "(1264, 2)\n",
      "(1336, 2)\n",
      "(388, 2)\n",
      "(784, 2)\n",
      "(783, 2)\n"
     ]
    }
   ],
   "source": [
    "indicators = [\n",
    "            GDP, CPIAUCSL, UNRATE, FEDFUNDS, GS10,\n",
    "        INDPRO, PPIACO, RSXFS, HOUST, PSAVERT\n",
    "    ]\n",
    "for ind in indicators:\n",
    "    print(ind.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b942274-e36d-44d9-9302-da44cf0948b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1336, 11)\n"
     ]
    }
   ],
   "source": [
    "for ind in indicators:\n",
    "    ind.columns = ['Date', ind.columns[1]]\n",
    "\n",
    "# Merging based on the 'Date' column\n",
    "merged_df = GDP\n",
    "for ind in indicators[1:]:\n",
    "    merged_df = pd.merge(merged_df, ind, on='Date', how='outer')\n",
    "\n",
    "# Check the shape of the merged DataFrame\n",
    "print(merged_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd89e782-15f3-4fad-bca6-7afd7434c4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6ddad1b-88ae-4a9e-985e-866a7e6f58c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(129, 11)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afa0f245-a74d-4cc8-8315-700272533607",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.to_csv(\"./data/indicators.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a56420a-7526-4b17-9bce-2ab91cf15ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_collection.collect_num_data import get_economic_indicators, get_historical_prices, get_dividends_and_earnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "969974ec-fd92-4c2d-9379-d82b3393cf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_csv = \"portfolio_files/Apr-2025 portfolio.csv\"\n",
    "portfolio_df = pd.read_csv(portfolio_csv)\n",
    "portfolio_symbols = portfolio_df[\"Symbol\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb1c1294-0e3d-4801-baf7-4190e1d4e260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "741723fd-ef92-4a7c-8a74-91fe0c166d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "for symbol in portfolio_symbols:\n",
    "    hist_data = get_historical_prices(symbol)\n",
    "    try:\n",
    "        hist_data.index = pd.to_datetime(hist_data.index)\n",
    "        hist_data.index = hist_data.index.strftime('%Y-%m-%d')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    hist_data.to_csv(f\"./data/historical/{symbol}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5df21cab-89cc-43f0-9bfe-5b2c8c686592",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Open', 'High', 'Low', 'Close', 'Volume'], dtype='object')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = get_historical_prices(\"GOOG\")\n",
    "tmp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ed5cc766-b554-45a5-9146-495aef0c73ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators_df = pd.read_csv(\"data/indicators.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ceac414-c83a-4fb8-bf5a-4877558f9b1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Date</th>\n",
       "      <th>GDP</th>\n",
       "      <th>CPIAUCSL</th>\n",
       "      <th>UNRATE</th>\n",
       "      <th>FEDFUNDS</th>\n",
       "      <th>GS10</th>\n",
       "      <th>INDPRO</th>\n",
       "      <th>PPIACO</th>\n",
       "      <th>RSXFS</th>\n",
       "      <th>HOUST</th>\n",
       "      <th>PSAVERT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>948</td>\n",
       "      <td>1992-01-01</td>\n",
       "      <td>6363.102</td>\n",
       "      <td>138.300</td>\n",
       "      <td>7.3</td>\n",
       "      <td>4.03</td>\n",
       "      <td>7.03</td>\n",
       "      <td>61.4898</td>\n",
       "      <td>115.600</td>\n",
       "      <td>146925.0</td>\n",
       "      <td>1176.0</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>951</td>\n",
       "      <td>1992-04-01</td>\n",
       "      <td>6470.763</td>\n",
       "      <td>139.400</td>\n",
       "      <td>7.4</td>\n",
       "      <td>3.73</td>\n",
       "      <td>7.48</td>\n",
       "      <td>62.9312</td>\n",
       "      <td>116.300</td>\n",
       "      <td>148032.0</td>\n",
       "      <td>1099.0</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>954</td>\n",
       "      <td>1992-07-01</td>\n",
       "      <td>6566.641</td>\n",
       "      <td>140.500</td>\n",
       "      <td>7.7</td>\n",
       "      <td>3.25</td>\n",
       "      <td>6.84</td>\n",
       "      <td>63.7408</td>\n",
       "      <td>117.900</td>\n",
       "      <td>150761.0</td>\n",
       "      <td>1139.0</td>\n",
       "      <td>9.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>957</td>\n",
       "      <td>1992-10-01</td>\n",
       "      <td>6680.803</td>\n",
       "      <td>141.700</td>\n",
       "      <td>7.3</td>\n",
       "      <td>3.10</td>\n",
       "      <td>6.59</td>\n",
       "      <td>64.0282</td>\n",
       "      <td>118.100</td>\n",
       "      <td>153521.0</td>\n",
       "      <td>1244.0</td>\n",
       "      <td>7.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>960</td>\n",
       "      <td>1993-01-01</td>\n",
       "      <td>6729.459</td>\n",
       "      <td>142.800</td>\n",
       "      <td>7.3</td>\n",
       "      <td>3.02</td>\n",
       "      <td>6.60</td>\n",
       "      <td>64.6256</td>\n",
       "      <td>118.000</td>\n",
       "      <td>157555.0</td>\n",
       "      <td>1210.0</td>\n",
       "      <td>8.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>1320</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>26813.601</td>\n",
       "      <td>300.356</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.33</td>\n",
       "      <td>3.53</td>\n",
       "      <td>102.5478</td>\n",
       "      <td>260.227</td>\n",
       "      <td>603496.0</td>\n",
       "      <td>1361.0</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>1323</td>\n",
       "      <td>2023-04-01</td>\n",
       "      <td>27063.012</td>\n",
       "      <td>303.032</td>\n",
       "      <td>3.4</td>\n",
       "      <td>4.83</td>\n",
       "      <td>3.46</td>\n",
       "      <td>103.1512</td>\n",
       "      <td>256.908</td>\n",
       "      <td>595397.0</td>\n",
       "      <td>1368.0</td>\n",
       "      <td>5.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>1326</td>\n",
       "      <td>2023-07-01</td>\n",
       "      <td>27610.128</td>\n",
       "      <td>304.628</td>\n",
       "      <td>3.5</td>\n",
       "      <td>5.12</td>\n",
       "      <td>3.90</td>\n",
       "      <td>103.1900</td>\n",
       "      <td>253.835</td>\n",
       "      <td>599037.0</td>\n",
       "      <td>1473.0</td>\n",
       "      <td>4.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>1329</td>\n",
       "      <td>2023-10-01</td>\n",
       "      <td>27956.998</td>\n",
       "      <td>307.531</td>\n",
       "      <td>3.8</td>\n",
       "      <td>5.33</td>\n",
       "      <td>4.80</td>\n",
       "      <td>102.5683</td>\n",
       "      <td>255.192</td>\n",
       "      <td>606596.0</td>\n",
       "      <td>1365.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>1332</td>\n",
       "      <td>2024-01-01</td>\n",
       "      <td>28284.498</td>\n",
       "      <td>309.685</td>\n",
       "      <td>3.7</td>\n",
       "      <td>5.33</td>\n",
       "      <td>4.06</td>\n",
       "      <td>101.7987</td>\n",
       "      <td>251.245</td>\n",
       "      <td>601921.0</td>\n",
       "      <td>1376.0</td>\n",
       "      <td>4.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0        Date        GDP  CPIAUCSL  UNRATE  FEDFUNDS  GS10  \\\n",
       "0           948  1992-01-01   6363.102   138.300     7.3      4.03  7.03   \n",
       "1           951  1992-04-01   6470.763   139.400     7.4      3.73  7.48   \n",
       "2           954  1992-07-01   6566.641   140.500     7.7      3.25  6.84   \n",
       "3           957  1992-10-01   6680.803   141.700     7.3      3.10  6.59   \n",
       "4           960  1993-01-01   6729.459   142.800     7.3      3.02  6.60   \n",
       "..          ...         ...        ...       ...     ...       ...   ...   \n",
       "124        1320  2023-01-01  26813.601   300.356     3.4      4.33  3.53   \n",
       "125        1323  2023-04-01  27063.012   303.032     3.4      4.83  3.46   \n",
       "126        1326  2023-07-01  27610.128   304.628     3.5      5.12  3.90   \n",
       "127        1329  2023-10-01  27956.998   307.531     3.8      5.33  4.80   \n",
       "128        1332  2024-01-01  28284.498   309.685     3.7      5.33  4.06   \n",
       "\n",
       "       INDPRO   PPIACO     RSXFS   HOUST  PSAVERT  \n",
       "0     61.4898  115.600  146925.0  1176.0      9.5  \n",
       "1     62.9312  116.300  148032.0  1099.0      9.8  \n",
       "2     63.7408  117.900  150761.0  1139.0      9.5  \n",
       "3     64.0282  118.100  153521.0  1244.0      7.9  \n",
       "4     64.6256  118.000  157555.0  1210.0      8.5  \n",
       "..        ...      ...       ...     ...      ...  \n",
       "124  102.5478  260.227  603496.0  1361.0      4.4  \n",
       "125  103.1512  256.908  595397.0  1368.0      5.2  \n",
       "126  103.1900  253.835  599037.0  1473.0      4.4  \n",
       "127  102.5683  255.192  606596.0  1365.0      4.0  \n",
       "128  101.7987  251.245  601921.0  1376.0      4.1  \n",
       "\n",
       "[129 rows x 12 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indicators_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "16dab3e4-dd8e-4b3d-b1c0-fb8f4f504cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "goog_df =  pd.read_csv(\"data/historical/GOOG.csv\")\n",
    "jpm_df =  pd.read_csv(\"data/historical/JPM.csv\")\n",
    "msft_df =  pd.read_csv(\"data/historical/MSFT.csv\")\n",
    "nvda_df =  pd.read_csv(\"data/historical/NVDA.csv\")\n",
    "voo_df =  pd.read_csv(\"data/historical/VOO.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2117943f-9ba4-4ad8-adc7-3d640c51ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = {\n",
    "    'GOOG': goog_df,\n",
    "    'JPM': jpm_df,\n",
    "    'MSFT':msft_df,\n",
    "    'NVDA': nvda_df,\n",
    "    'VOO':voo_df\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1ca6074-752a-43ce-a8cd-5630e4dd9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df = pd.concat([df.set_index('Date') for df in dataframes.values()], axis=1, keys=dataframes.keys())\n",
    "combined_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "81cc7ace-0e4d-4cd6-8ab2-084d2c297a7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th colspan=\"5\" halign=\"left\">GOOG</th>\n",
       "      <th colspan=\"4\" halign=\"left\">JPM</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"5\" halign=\"left\">NVDA</th>\n",
       "      <th colspan=\"5\" halign=\"left\">VOO</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>...</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004-08-19</td>\n",
       "      <td>2.487833</td>\n",
       "      <td>2.588839</td>\n",
       "      <td>2.387325</td>\n",
       "      <td>2.496292</td>\n",
       "      <td>897427216.0</td>\n",
       "      <td>22.305618</td>\n",
       "      <td>22.305618</td>\n",
       "      <td>22.032036</td>\n",
       "      <td>22.119350</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088895</td>\n",
       "      <td>0.090500</td>\n",
       "      <td>0.087748</td>\n",
       "      <td>0.089124</td>\n",
       "      <td>7.244040e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-08-20</td>\n",
       "      <td>2.512960</td>\n",
       "      <td>2.713729</td>\n",
       "      <td>2.500273</td>\n",
       "      <td>2.694573</td>\n",
       "      <td>458857488.0</td>\n",
       "      <td>22.101885</td>\n",
       "      <td>22.585017</td>\n",
       "      <td>22.072779</td>\n",
       "      <td>22.497705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088666</td>\n",
       "      <td>0.095697</td>\n",
       "      <td>0.088359</td>\n",
       "      <td>0.094398</td>\n",
       "      <td>1.199040e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-08-23</td>\n",
       "      <td>2.755276</td>\n",
       "      <td>2.823193</td>\n",
       "      <td>2.712983</td>\n",
       "      <td>2.721690</td>\n",
       "      <td>366857939.0</td>\n",
       "      <td>22.555915</td>\n",
       "      <td>22.631587</td>\n",
       "      <td>22.392931</td>\n",
       "      <td>22.439497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.095926</td>\n",
       "      <td>0.097455</td>\n",
       "      <td>0.095392</td>\n",
       "      <td>0.096462</td>\n",
       "      <td>8.000280e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-08-24</td>\n",
       "      <td>2.767466</td>\n",
       "      <td>2.776422</td>\n",
       "      <td>2.576649</td>\n",
       "      <td>2.608991</td>\n",
       "      <td>306396159.0</td>\n",
       "      <td>22.555910</td>\n",
       "      <td>22.614118</td>\n",
       "      <td>22.317254</td>\n",
       "      <td>22.462776</td>\n",
       "      <td>...</td>\n",
       "      <td>0.097838</td>\n",
       "      <td>0.098296</td>\n",
       "      <td>0.091035</td>\n",
       "      <td>0.092869</td>\n",
       "      <td>9.394200e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-08-25</td>\n",
       "      <td>2.611230</td>\n",
       "      <td>2.686860</td>\n",
       "      <td>2.584361</td>\n",
       "      <td>2.637103</td>\n",
       "      <td>184645512.0</td>\n",
       "      <td>22.468602</td>\n",
       "      <td>23.068154</td>\n",
       "      <td>22.468602</td>\n",
       "      <td>22.957556</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094933</td>\n",
       "      <td>0.097531</td>\n",
       "      <td>0.093175</td>\n",
       "      <td>0.096844</td>\n",
       "      <td>7.213080e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11208</th>\n",
       "      <td>2004-08-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.432474</td>\n",
       "      <td>21.496504</td>\n",
       "      <td>21.246206</td>\n",
       "      <td>21.461578</td>\n",
       "      <td>...</td>\n",
       "      <td>0.078881</td>\n",
       "      <td>0.080792</td>\n",
       "      <td>0.078194</td>\n",
       "      <td>0.080257</td>\n",
       "      <td>8.157000e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11209</th>\n",
       "      <td>2004-08-16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.508145</td>\n",
       "      <td>22.002919</td>\n",
       "      <td>21.496503</td>\n",
       "      <td>21.967995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079340</td>\n",
       "      <td>0.081480</td>\n",
       "      <td>0.078882</td>\n",
       "      <td>0.079493</td>\n",
       "      <td>5.317320e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11210</th>\n",
       "      <td>2004-08-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.090238</td>\n",
       "      <td>22.264864</td>\n",
       "      <td>22.037850</td>\n",
       "      <td>22.078596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081251</td>\n",
       "      <td>0.084309</td>\n",
       "      <td>0.080487</td>\n",
       "      <td>0.083850</td>\n",
       "      <td>7.449120e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11211</th>\n",
       "      <td>2004-08-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.061139</td>\n",
       "      <td>22.334718</td>\n",
       "      <td>22.020393</td>\n",
       "      <td>22.305614</td>\n",
       "      <td>...</td>\n",
       "      <td>0.084232</td>\n",
       "      <td>0.089430</td>\n",
       "      <td>0.083544</td>\n",
       "      <td>0.089430</td>\n",
       "      <td>8.724240e+08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11212</th>\n",
       "      <td>2024-09-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>217.600006</td>\n",
       "      <td>218.740005</td>\n",
       "      <td>211.089996</td>\n",
       "      <td>212.850006</td>\n",
       "      <td>...</td>\n",
       "      <td>108.040001</td>\n",
       "      <td>108.150002</td>\n",
       "      <td>100.949997</td>\n",
       "      <td>103.209297</td>\n",
       "      <td>3.321699e+08</td>\n",
       "      <td>505.339996</td>\n",
       "      <td>506.940002</td>\n",
       "      <td>495.799988</td>\n",
       "      <td>497.380005</td>\n",
       "      <td>5159002.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11213 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Date      GOOG                                             \\\n",
       "                       Open      High       Low     Close       Volume   \n",
       "0      2004-08-19  2.487833  2.588839  2.387325  2.496292  897427216.0   \n",
       "1      2004-08-20  2.512960  2.713729  2.500273  2.694573  458857488.0   \n",
       "2      2004-08-23  2.755276  2.823193  2.712983  2.721690  366857939.0   \n",
       "3      2004-08-24  2.767466  2.776422  2.576649  2.608991  306396159.0   \n",
       "4      2004-08-25  2.611230  2.686860  2.584361  2.637103  184645512.0   \n",
       "...           ...       ...       ...       ...       ...          ...   \n",
       "11208  2004-08-13       NaN       NaN       NaN       NaN          NaN   \n",
       "11209  2004-08-16       NaN       NaN       NaN       NaN          NaN   \n",
       "11210  2004-08-17       NaN       NaN       NaN       NaN          NaN   \n",
       "11211  2004-08-18       NaN       NaN       NaN       NaN          NaN   \n",
       "11212  2024-09-06       NaN       NaN       NaN       NaN          NaN   \n",
       "\n",
       "              JPM                                      ...        NVDA  \\\n",
       "             Open        High         Low       Close  ...        Open   \n",
       "0       22.305618   22.305618   22.032036   22.119350  ...    0.088895   \n",
       "1       22.101885   22.585017   22.072779   22.497705  ...    0.088666   \n",
       "2       22.555915   22.631587   22.392931   22.439497  ...    0.095926   \n",
       "3       22.555910   22.614118   22.317254   22.462776  ...    0.097838   \n",
       "4       22.468602   23.068154   22.468602   22.957556  ...    0.094933   \n",
       "...           ...         ...         ...         ...  ...         ...   \n",
       "11208   21.432474   21.496504   21.246206   21.461578  ...    0.078881   \n",
       "11209   21.508145   22.002919   21.496503   21.967995  ...    0.079340   \n",
       "11210   22.090238   22.264864   22.037850   22.078596  ...    0.081251   \n",
       "11211   22.061139   22.334718   22.020393   22.305614  ...    0.084232   \n",
       "11212  217.600006  218.740005  211.089996  212.850006  ...  108.040001   \n",
       "\n",
       "                                                                VOO  \\\n",
       "             High         Low       Close        Volume        Open   \n",
       "0        0.090500    0.087748    0.089124  7.244040e+08         NaN   \n",
       "1        0.095697    0.088359    0.094398  1.199040e+09         NaN   \n",
       "2        0.097455    0.095392    0.096462  8.000280e+08         NaN   \n",
       "3        0.098296    0.091035    0.092869  9.394200e+08         NaN   \n",
       "4        0.097531    0.093175    0.096844  7.213080e+08         NaN   \n",
       "...           ...         ...         ...           ...         ...   \n",
       "11208    0.080792    0.078194    0.080257  8.157000e+08         NaN   \n",
       "11209    0.081480    0.078882    0.079493  5.317320e+08         NaN   \n",
       "11210    0.084309    0.080487    0.083850  7.449120e+08         NaN   \n",
       "11211    0.089430    0.083544    0.089430  8.724240e+08         NaN   \n",
       "11212  108.150002  100.949997  103.209297  3.321699e+08  505.339996   \n",
       "\n",
       "                                                      \n",
       "             High         Low       Close     Volume  \n",
       "0             NaN         NaN         NaN        NaN  \n",
       "1             NaN         NaN         NaN        NaN  \n",
       "2             NaN         NaN         NaN        NaN  \n",
       "3             NaN         NaN         NaN        NaN  \n",
       "4             NaN         NaN         NaN        NaN  \n",
       "...           ...         ...         ...        ...  \n",
       "11208         NaN         NaN         NaN        NaN  \n",
       "11209         NaN         NaN         NaN        NaN  \n",
       "11210         NaN         NaN         NaN        NaN  \n",
       "11211         NaN         NaN         NaN        NaN  \n",
       "11212  506.940002  495.799988  497.380005  5159002.0  \n",
       "\n",
       "[11213 rows x 26 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7eb64787-5d19-4d25-98e7-30fef577118b",
   "metadata": {},
   "outputs": [],
   "source": [
    "indicators_df['Date'] = pd.to_datetime(indicators_df['Date'])\n",
    "combined_df['Date'] = pd.to_datetime(combined_df['Date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "af6ef1f8-7b52-4114-b5fb-a935116c5f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten MultiIndex columns\n",
    "combined_df.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col for col in combined_df.columns]\n",
    "\n",
    "# Ensure 'Date' column is correctly named\n",
    "if 'Date_' in combined_df.columns:\n",
    "    combined_df.rename(columns={'Date_': 'Date'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e42e3c5-3bdd-4b14-9b92-b9dc42b18085",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df = pd.merge(combined_df, indicators_df, on='Date', how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a81ee94a-9466-4398-a57f-5bb57fd39912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>GOOG_Open</th>\n",
       "      <th>GOOG_High</th>\n",
       "      <th>GOOG_Low</th>\n",
       "      <th>GOOG_Close</th>\n",
       "      <th>GOOG_Volume</th>\n",
       "      <th>JPM_Open</th>\n",
       "      <th>JPM_High</th>\n",
       "      <th>JPM_Low</th>\n",
       "      <th>JPM_Close</th>\n",
       "      <th>...</th>\n",
       "      <th>GDP</th>\n",
       "      <th>CPIAUCSL</th>\n",
       "      <th>UNRATE</th>\n",
       "      <th>FEDFUNDS</th>\n",
       "      <th>GS10</th>\n",
       "      <th>INDPRO</th>\n",
       "      <th>PPIACO</th>\n",
       "      <th>RSXFS</th>\n",
       "      <th>HOUST</th>\n",
       "      <th>PSAVERT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004-08-19</td>\n",
       "      <td>2.487833</td>\n",
       "      <td>2.588839</td>\n",
       "      <td>2.387325</td>\n",
       "      <td>2.496292</td>\n",
       "      <td>897427216.0</td>\n",
       "      <td>22.305618</td>\n",
       "      <td>22.305618</td>\n",
       "      <td>22.032036</td>\n",
       "      <td>22.119350</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-08-20</td>\n",
       "      <td>2.512960</td>\n",
       "      <td>2.713729</td>\n",
       "      <td>2.500273</td>\n",
       "      <td>2.694573</td>\n",
       "      <td>458857488.0</td>\n",
       "      <td>22.101885</td>\n",
       "      <td>22.585017</td>\n",
       "      <td>22.072779</td>\n",
       "      <td>22.497705</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-08-23</td>\n",
       "      <td>2.755276</td>\n",
       "      <td>2.823193</td>\n",
       "      <td>2.712983</td>\n",
       "      <td>2.721690</td>\n",
       "      <td>366857939.0</td>\n",
       "      <td>22.555915</td>\n",
       "      <td>22.631587</td>\n",
       "      <td>22.392931</td>\n",
       "      <td>22.439497</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-08-24</td>\n",
       "      <td>2.767466</td>\n",
       "      <td>2.776422</td>\n",
       "      <td>2.576649</td>\n",
       "      <td>2.608991</td>\n",
       "      <td>306396159.0</td>\n",
       "      <td>22.555910</td>\n",
       "      <td>22.614118</td>\n",
       "      <td>22.317254</td>\n",
       "      <td>22.462776</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-08-25</td>\n",
       "      <td>2.611230</td>\n",
       "      <td>2.686860</td>\n",
       "      <td>2.584361</td>\n",
       "      <td>2.637103</td>\n",
       "      <td>184645512.0</td>\n",
       "      <td>22.468602</td>\n",
       "      <td>23.068154</td>\n",
       "      <td>22.468602</td>\n",
       "      <td>22.957556</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11208</th>\n",
       "      <td>2004-08-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.432474</td>\n",
       "      <td>21.496504</td>\n",
       "      <td>21.246206</td>\n",
       "      <td>21.461578</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11209</th>\n",
       "      <td>2004-08-16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21.508145</td>\n",
       "      <td>22.002919</td>\n",
       "      <td>21.496503</td>\n",
       "      <td>21.967995</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11210</th>\n",
       "      <td>2004-08-17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.090238</td>\n",
       "      <td>22.264864</td>\n",
       "      <td>22.037850</td>\n",
       "      <td>22.078596</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11211</th>\n",
       "      <td>2004-08-18</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22.061139</td>\n",
       "      <td>22.334718</td>\n",
       "      <td>22.020393</td>\n",
       "      <td>22.305614</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11212</th>\n",
       "      <td>2024-09-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>217.600006</td>\n",
       "      <td>218.740005</td>\n",
       "      <td>211.089996</td>\n",
       "      <td>212.850006</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11213 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  GOOG_Open  GOOG_High  GOOG_Low  GOOG_Close  GOOG_Volume  \\\n",
       "0     2004-08-19   2.487833   2.588839  2.387325    2.496292  897427216.0   \n",
       "1     2004-08-20   2.512960   2.713729  2.500273    2.694573  458857488.0   \n",
       "2     2004-08-23   2.755276   2.823193  2.712983    2.721690  366857939.0   \n",
       "3     2004-08-24   2.767466   2.776422  2.576649    2.608991  306396159.0   \n",
       "4     2004-08-25   2.611230   2.686860  2.584361    2.637103  184645512.0   \n",
       "...          ...        ...        ...       ...         ...          ...   \n",
       "11208 2004-08-13        NaN        NaN       NaN         NaN          NaN   \n",
       "11209 2004-08-16        NaN        NaN       NaN         NaN          NaN   \n",
       "11210 2004-08-17        NaN        NaN       NaN         NaN          NaN   \n",
       "11211 2004-08-18        NaN        NaN       NaN         NaN          NaN   \n",
       "11212 2024-09-06        NaN        NaN       NaN         NaN          NaN   \n",
       "\n",
       "         JPM_Open    JPM_High     JPM_Low   JPM_Close  ...  GDP  CPIAUCSL  \\\n",
       "0       22.305618   22.305618   22.032036   22.119350  ...  NaN       NaN   \n",
       "1       22.101885   22.585017   22.072779   22.497705  ...  NaN       NaN   \n",
       "2       22.555915   22.631587   22.392931   22.439497  ...  NaN       NaN   \n",
       "3       22.555910   22.614118   22.317254   22.462776  ...  NaN       NaN   \n",
       "4       22.468602   23.068154   22.468602   22.957556  ...  NaN       NaN   \n",
       "...           ...         ...         ...         ...  ...  ...       ...   \n",
       "11208   21.432474   21.496504   21.246206   21.461578  ...  NaN       NaN   \n",
       "11209   21.508145   22.002919   21.496503   21.967995  ...  NaN       NaN   \n",
       "11210   22.090238   22.264864   22.037850   22.078596  ...  NaN       NaN   \n",
       "11211   22.061139   22.334718   22.020393   22.305614  ...  NaN       NaN   \n",
       "11212  217.600006  218.740005  211.089996  212.850006  ...  NaN       NaN   \n",
       "\n",
       "       UNRATE  FEDFUNDS  GS10  INDPRO  PPIACO  RSXFS  HOUST  PSAVERT  \n",
       "0         NaN       NaN   NaN     NaN     NaN    NaN    NaN      NaN  \n",
       "1         NaN       NaN   NaN     NaN     NaN    NaN    NaN      NaN  \n",
       "2         NaN       NaN   NaN     NaN     NaN    NaN    NaN      NaN  \n",
       "3         NaN       NaN   NaN     NaN     NaN    NaN    NaN      NaN  \n",
       "4         NaN       NaN   NaN     NaN     NaN    NaN    NaN      NaN  \n",
       "...       ...       ...   ...     ...     ...    ...    ...      ...  \n",
       "11208     NaN       NaN   NaN     NaN     NaN    NaN    NaN      NaN  \n",
       "11209     NaN       NaN   NaN     NaN     NaN    NaN    NaN      NaN  \n",
       "11210     NaN       NaN   NaN     NaN     NaN    NaN    NaN      NaN  \n",
       "11211     NaN       NaN   NaN     NaN     NaN    NaN    NaN      NaN  \n",
       "11212     NaN       NaN   NaN     NaN     NaN    NaN    NaN      NaN  \n",
       "\n",
       "[11213 rows x 37 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "406e510b-1291-4375-9999-c390f49263c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "imputer = KNNImputer(n_neighbors=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a47dc33-56df-4b60-b72e-ff296f503ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_df_imputed = full_df.copy()\n",
    "full_df_imputed.iloc[:, 1:] = imputer.fit_transform(full_df_imputed.iloc[:, 1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "275ec91e-8030-4f78-9939-392f3afdc271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>GOOG_Open</th>\n",
       "      <th>GOOG_High</th>\n",
       "      <th>GOOG_Low</th>\n",
       "      <th>GOOG_Close</th>\n",
       "      <th>GOOG_Volume</th>\n",
       "      <th>JPM_Open</th>\n",
       "      <th>JPM_High</th>\n",
       "      <th>JPM_Low</th>\n",
       "      <th>JPM_Close</th>\n",
       "      <th>...</th>\n",
       "      <th>GDP</th>\n",
       "      <th>CPIAUCSL</th>\n",
       "      <th>UNRATE</th>\n",
       "      <th>FEDFUNDS</th>\n",
       "      <th>GS10</th>\n",
       "      <th>INDPRO</th>\n",
       "      <th>PPIACO</th>\n",
       "      <th>RSXFS</th>\n",
       "      <th>HOUST</th>\n",
       "      <th>PSAVERT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2004-08-19</td>\n",
       "      <td>2.487833</td>\n",
       "      <td>2.588839</td>\n",
       "      <td>2.387325</td>\n",
       "      <td>2.496292</td>\n",
       "      <td>897427216.0</td>\n",
       "      <td>22.305618</td>\n",
       "      <td>22.305618</td>\n",
       "      <td>22.032036</td>\n",
       "      <td>22.119350</td>\n",
       "      <td>...</td>\n",
       "      <td>7502.7374</td>\n",
       "      <td>149.600</td>\n",
       "      <td>6.36</td>\n",
       "      <td>4.056</td>\n",
       "      <td>6.218</td>\n",
       "      <td>70.67188</td>\n",
       "      <td>121.60</td>\n",
       "      <td>175999.0</td>\n",
       "      <td>1389.0</td>\n",
       "      <td>7.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2004-08-20</td>\n",
       "      <td>2.512960</td>\n",
       "      <td>2.713729</td>\n",
       "      <td>2.500273</td>\n",
       "      <td>2.694573</td>\n",
       "      <td>458857488.0</td>\n",
       "      <td>22.101885</td>\n",
       "      <td>22.585017</td>\n",
       "      <td>22.072779</td>\n",
       "      <td>22.497705</td>\n",
       "      <td>...</td>\n",
       "      <td>7502.7374</td>\n",
       "      <td>149.600</td>\n",
       "      <td>6.36</td>\n",
       "      <td>4.056</td>\n",
       "      <td>6.218</td>\n",
       "      <td>70.67188</td>\n",
       "      <td>121.60</td>\n",
       "      <td>175999.0</td>\n",
       "      <td>1389.0</td>\n",
       "      <td>7.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2004-08-23</td>\n",
       "      <td>2.755276</td>\n",
       "      <td>2.823193</td>\n",
       "      <td>2.712983</td>\n",
       "      <td>2.721690</td>\n",
       "      <td>366857939.0</td>\n",
       "      <td>22.555915</td>\n",
       "      <td>22.631587</td>\n",
       "      <td>22.392931</td>\n",
       "      <td>22.439497</td>\n",
       "      <td>...</td>\n",
       "      <td>7742.8652</td>\n",
       "      <td>152.280</td>\n",
       "      <td>5.98</td>\n",
       "      <td>4.462</td>\n",
       "      <td>6.238</td>\n",
       "      <td>72.77582</td>\n",
       "      <td>123.46</td>\n",
       "      <td>184002.0</td>\n",
       "      <td>1411.8</td>\n",
       "      <td>6.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2004-08-24</td>\n",
       "      <td>2.767466</td>\n",
       "      <td>2.776422</td>\n",
       "      <td>2.576649</td>\n",
       "      <td>2.608991</td>\n",
       "      <td>306396159.0</td>\n",
       "      <td>22.555910</td>\n",
       "      <td>22.614118</td>\n",
       "      <td>22.317254</td>\n",
       "      <td>22.462776</td>\n",
       "      <td>...</td>\n",
       "      <td>7472.4578</td>\n",
       "      <td>149.400</td>\n",
       "      <td>6.32</td>\n",
       "      <td>4.038</td>\n",
       "      <td>6.254</td>\n",
       "      <td>70.72818</td>\n",
       "      <td>121.60</td>\n",
       "      <td>175757.4</td>\n",
       "      <td>1357.4</td>\n",
       "      <td>7.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2004-08-25</td>\n",
       "      <td>2.611230</td>\n",
       "      <td>2.686860</td>\n",
       "      <td>2.584361</td>\n",
       "      <td>2.637103</td>\n",
       "      <td>184645512.0</td>\n",
       "      <td>22.468602</td>\n",
       "      <td>23.068154</td>\n",
       "      <td>22.468602</td>\n",
       "      <td>22.957556</td>\n",
       "      <td>...</td>\n",
       "      <td>8312.6666</td>\n",
       "      <td>157.320</td>\n",
       "      <td>5.30</td>\n",
       "      <td>5.004</td>\n",
       "      <td>6.078</td>\n",
       "      <td>77.61714</td>\n",
       "      <td>125.36</td>\n",
       "      <td>197787.2</td>\n",
       "      <td>1476.2</td>\n",
       "      <td>6.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11208</th>\n",
       "      <td>2004-08-13</td>\n",
       "      <td>9.351367</td>\n",
       "      <td>9.432421</td>\n",
       "      <td>9.264442</td>\n",
       "      <td>9.322409</td>\n",
       "      <td>247552422.0</td>\n",
       "      <td>21.432474</td>\n",
       "      <td>21.496504</td>\n",
       "      <td>21.246206</td>\n",
       "      <td>21.461578</td>\n",
       "      <td>...</td>\n",
       "      <td>7472.4578</td>\n",
       "      <td>149.400</td>\n",
       "      <td>6.32</td>\n",
       "      <td>4.038</td>\n",
       "      <td>6.254</td>\n",
       "      <td>70.72818</td>\n",
       "      <td>121.60</td>\n",
       "      <td>175757.4</td>\n",
       "      <td>1357.4</td>\n",
       "      <td>7.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11209</th>\n",
       "      <td>2004-08-16</td>\n",
       "      <td>10.168621</td>\n",
       "      <td>10.312218</td>\n",
       "      <td>9.998105</td>\n",
       "      <td>10.138369</td>\n",
       "      <td>470500164.6</td>\n",
       "      <td>21.508145</td>\n",
       "      <td>22.002919</td>\n",
       "      <td>21.496503</td>\n",
       "      <td>21.967995</td>\n",
       "      <td>...</td>\n",
       "      <td>8948.9382</td>\n",
       "      <td>162.560</td>\n",
       "      <td>5.40</td>\n",
       "      <td>4.176</td>\n",
       "      <td>5.496</td>\n",
       "      <td>80.62768</td>\n",
       "      <td>127.12</td>\n",
       "      <td>211989.2</td>\n",
       "      <td>1510.4</td>\n",
       "      <td>6.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11210</th>\n",
       "      <td>2004-08-17</td>\n",
       "      <td>8.640643</td>\n",
       "      <td>8.696619</td>\n",
       "      <td>8.567551</td>\n",
       "      <td>8.629448</td>\n",
       "      <td>213490024.8</td>\n",
       "      <td>22.090238</td>\n",
       "      <td>22.264864</td>\n",
       "      <td>22.037850</td>\n",
       "      <td>22.078596</td>\n",
       "      <td>...</td>\n",
       "      <td>8545.1832</td>\n",
       "      <td>158.700</td>\n",
       "      <td>5.10</td>\n",
       "      <td>4.938</td>\n",
       "      <td>5.610</td>\n",
       "      <td>79.86140</td>\n",
       "      <td>124.56</td>\n",
       "      <td>202446.4</td>\n",
       "      <td>1524.8</td>\n",
       "      <td>6.34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11211</th>\n",
       "      <td>2004-08-18</td>\n",
       "      <td>8.855044</td>\n",
       "      <td>8.939680</td>\n",
       "      <td>8.767921</td>\n",
       "      <td>8.838277</td>\n",
       "      <td>286163806.6</td>\n",
       "      <td>22.061139</td>\n",
       "      <td>22.334718</td>\n",
       "      <td>22.020393</td>\n",
       "      <td>22.305614</td>\n",
       "      <td>...</td>\n",
       "      <td>9075.4106</td>\n",
       "      <td>163.040</td>\n",
       "      <td>5.40</td>\n",
       "      <td>4.032</td>\n",
       "      <td>5.376</td>\n",
       "      <td>81.29254</td>\n",
       "      <td>127.44</td>\n",
       "      <td>214431.0</td>\n",
       "      <td>1578.6</td>\n",
       "      <td>6.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11212</th>\n",
       "      <td>2024-09-06</td>\n",
       "      <td>115.948524</td>\n",
       "      <td>116.838441</td>\n",
       "      <td>114.974020</td>\n",
       "      <td>115.838497</td>\n",
       "      <td>16462160.0</td>\n",
       "      <td>217.600006</td>\n",
       "      <td>218.740005</td>\n",
       "      <td>211.089996</td>\n",
       "      <td>212.850006</td>\n",
       "      <td>...</td>\n",
       "      <td>16895.2388</td>\n",
       "      <td>225.444</td>\n",
       "      <td>6.50</td>\n",
       "      <td>1.178</td>\n",
       "      <td>2.684</td>\n",
       "      <td>94.36160</td>\n",
       "      <td>180.50</td>\n",
       "      <td>370425.2</td>\n",
       "      <td>1295.4</td>\n",
       "      <td>8.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11213 rows × 37 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date   GOOG_Open   GOOG_High    GOOG_Low  GOOG_Close  GOOG_Volume  \\\n",
       "0     2004-08-19    2.487833    2.588839    2.387325    2.496292  897427216.0   \n",
       "1     2004-08-20    2.512960    2.713729    2.500273    2.694573  458857488.0   \n",
       "2     2004-08-23    2.755276    2.823193    2.712983    2.721690  366857939.0   \n",
       "3     2004-08-24    2.767466    2.776422    2.576649    2.608991  306396159.0   \n",
       "4     2004-08-25    2.611230    2.686860    2.584361    2.637103  184645512.0   \n",
       "...          ...         ...         ...         ...         ...          ...   \n",
       "11208 2004-08-13    9.351367    9.432421    9.264442    9.322409  247552422.0   \n",
       "11209 2004-08-16   10.168621   10.312218    9.998105   10.138369  470500164.6   \n",
       "11210 2004-08-17    8.640643    8.696619    8.567551    8.629448  213490024.8   \n",
       "11211 2004-08-18    8.855044    8.939680    8.767921    8.838277  286163806.6   \n",
       "11212 2024-09-06  115.948524  116.838441  114.974020  115.838497   16462160.0   \n",
       "\n",
       "         JPM_Open    JPM_High     JPM_Low   JPM_Close  ...         GDP  \\\n",
       "0       22.305618   22.305618   22.032036   22.119350  ...   7502.7374   \n",
       "1       22.101885   22.585017   22.072779   22.497705  ...   7502.7374   \n",
       "2       22.555915   22.631587   22.392931   22.439497  ...   7742.8652   \n",
       "3       22.555910   22.614118   22.317254   22.462776  ...   7472.4578   \n",
       "4       22.468602   23.068154   22.468602   22.957556  ...   8312.6666   \n",
       "...           ...         ...         ...         ...  ...         ...   \n",
       "11208   21.432474   21.496504   21.246206   21.461578  ...   7472.4578   \n",
       "11209   21.508145   22.002919   21.496503   21.967995  ...   8948.9382   \n",
       "11210   22.090238   22.264864   22.037850   22.078596  ...   8545.1832   \n",
       "11211   22.061139   22.334718   22.020393   22.305614  ...   9075.4106   \n",
       "11212  217.600006  218.740005  211.089996  212.850006  ...  16895.2388   \n",
       "\n",
       "       CPIAUCSL  UNRATE  FEDFUNDS   GS10    INDPRO  PPIACO     RSXFS   HOUST  \\\n",
       "0       149.600    6.36     4.056  6.218  70.67188  121.60  175999.0  1389.0   \n",
       "1       149.600    6.36     4.056  6.218  70.67188  121.60  175999.0  1389.0   \n",
       "2       152.280    5.98     4.462  6.238  72.77582  123.46  184002.0  1411.8   \n",
       "3       149.400    6.32     4.038  6.254  70.72818  121.60  175757.4  1357.4   \n",
       "4       157.320    5.30     5.004  6.078  77.61714  125.36  197787.2  1476.2   \n",
       "...         ...     ...       ...    ...       ...     ...       ...     ...   \n",
       "11208   149.400    6.32     4.038  6.254  70.72818  121.60  175757.4  1357.4   \n",
       "11209   162.560    5.40     4.176  5.496  80.62768  127.12  211989.2  1510.4   \n",
       "11210   158.700    5.10     4.938  5.610  79.86140  124.56  202446.4  1524.8   \n",
       "11211   163.040    5.40     4.032  5.376  81.29254  127.44  214431.0  1578.6   \n",
       "11212   225.444    6.50     1.178  2.684  94.36160  180.50  370425.2  1295.4   \n",
       "\n",
       "       PSAVERT  \n",
       "0         7.34  \n",
       "1         7.34  \n",
       "2         6.92  \n",
       "3         7.40  \n",
       "4         6.46  \n",
       "...        ...  \n",
       "11208     7.40  \n",
       "11209     6.16  \n",
       "11210     6.34  \n",
       "11211     6.02  \n",
       "11212     8.06  \n",
       "\n",
       "[11213 rows x 37 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b92002a-1bcc-4643-a9a1-37d6ce0734cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11213 entries, 0 to 11212\n",
      "Data columns (total 37 columns):\n",
      " #   Column       Non-Null Count  Dtype         \n",
      "---  ------       --------------  -----         \n",
      " 0   Date         11213 non-null  datetime64[ns]\n",
      " 1   GOOG_Open    11213 non-null  float64       \n",
      " 2   GOOG_High    11213 non-null  float64       \n",
      " 3   GOOG_Low     11213 non-null  float64       \n",
      " 4   GOOG_Close   11213 non-null  float64       \n",
      " 5   GOOG_Volume  11213 non-null  float64       \n",
      " 6   JPM_Open     11213 non-null  float64       \n",
      " 7   JPM_High     11213 non-null  float64       \n",
      " 8   JPM_Low      11213 non-null  float64       \n",
      " 9   JPM_Close    11213 non-null  float64       \n",
      " 10  JPM_Volume   11213 non-null  int64         \n",
      " 11  MSFT_Open    11213 non-null  float64       \n",
      " 12  MSFT_High    11213 non-null  float64       \n",
      " 13  MSFT_Low     11213 non-null  float64       \n",
      " 14  MSFT_Close   11213 non-null  float64       \n",
      " 15  MSFT_Volume  11213 non-null  float64       \n",
      " 16  NVDA_Open    11213 non-null  float64       \n",
      " 17  NVDA_High    11213 non-null  float64       \n",
      " 18  NVDA_Low     11213 non-null  float64       \n",
      " 19  NVDA_Close   11213 non-null  float64       \n",
      " 20  NVDA_Volume  11213 non-null  float64       \n",
      " 21  VOO_Open     11213 non-null  float64       \n",
      " 22  VOO_High     11213 non-null  float64       \n",
      " 23  VOO_Low      11213 non-null  float64       \n",
      " 24  VOO_Close    11213 non-null  float64       \n",
      " 25  VOO_Volume   11213 non-null  float64       \n",
      " 26  Unnamed: 0   11213 non-null  float64       \n",
      " 27  GDP          11213 non-null  float64       \n",
      " 28  CPIAUCSL     11213 non-null  float64       \n",
      " 29  UNRATE       11213 non-null  float64       \n",
      " 30  FEDFUNDS     11213 non-null  float64       \n",
      " 31  GS10         11213 non-null  float64       \n",
      " 32  INDPRO       11213 non-null  float64       \n",
      " 33  PPIACO       11213 non-null  float64       \n",
      " 34  RSXFS        11213 non-null  float64       \n",
      " 35  HOUST        11213 non-null  float64       \n",
      " 36  PSAVERT      11213 non-null  float64       \n",
      "dtypes: datetime64[ns](1), float64(35), int64(1)\n",
      "memory usage: 3.2 MB\n"
     ]
    }
   ],
   "source": [
    "full_df_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d36c4ffc-e371-42bb-bd02-3ccbbc0402c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022fbd15-95a4-4d8b-9be9-bb59abfa15d4",
   "metadata": {},
   "source": [
    "# Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a64dd79-847b-40e0-b4ce-23d3eaec1b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = ['GOOG', 'JPM', 'MSFT', 'NVDA', 'VOO']\n",
    "economic_indicators = ['GDP', 'CPIAUCSL', 'UNRATE', 'FEDFUNDS', 'GS10', 'INDPRO', 'PPIACO', 'RSXFS', 'HOUST', 'PSAVERT']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38873c2d-9ef0-4f4d-b8a8-23b61072d2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Returns for each stock (Relevant for modeling risk and capturing the stock's behavior over time)\n",
    "for stock in stocks:\n",
    "    full_df_imputed[f'{stock}_LogReturn'] = np.log(full_df_imputed[f'{stock}_Close'] / full_df_imputed[f'{stock}_Close'].shift(1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81b73b76-4afe-47ef-bda3-b522d647ae3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>GOOG_Open</th>\n",
       "      <th>GOOG_High</th>\n",
       "      <th>GOOG_Low</th>\n",
       "      <th>GOOG_Close</th>\n",
       "      <th>GOOG_Volume</th>\n",
       "      <th>JPM_Open</th>\n",
       "      <th>JPM_High</th>\n",
       "      <th>JPM_Low</th>\n",
       "      <th>JPM_Close</th>\n",
       "      <th>...</th>\n",
       "      <th>INDPRO</th>\n",
       "      <th>PPIACO</th>\n",
       "      <th>RSXFS</th>\n",
       "      <th>HOUST</th>\n",
       "      <th>PSAVERT</th>\n",
       "      <th>GOOG_LogReturn</th>\n",
       "      <th>JPM_LogReturn</th>\n",
       "      <th>MSFT_LogReturn</th>\n",
       "      <th>NVDA_LogReturn</th>\n",
       "      <th>VOO_LogReturn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11208</th>\n",
       "      <td>2004-08-13</td>\n",
       "      <td>9.351367</td>\n",
       "      <td>9.432421</td>\n",
       "      <td>9.264442</td>\n",
       "      <td>9.322409</td>\n",
       "      <td>247552422.0</td>\n",
       "      <td>21.432474</td>\n",
       "      <td>21.496504</td>\n",
       "      <td>21.246206</td>\n",
       "      <td>21.461578</td>\n",
       "      <td>...</td>\n",
       "      <td>70.72818</td>\n",
       "      <td>121.60</td>\n",
       "      <td>175757.4</td>\n",
       "      <td>1357.4</td>\n",
       "      <td>7.40</td>\n",
       "      <td>-1.281036</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.005195</td>\n",
       "      <td>0.040822</td>\n",
       "      <td>-0.319949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11209</th>\n",
       "      <td>2004-08-16</td>\n",
       "      <td>10.168621</td>\n",
       "      <td>10.312218</td>\n",
       "      <td>9.998105</td>\n",
       "      <td>10.138369</td>\n",
       "      <td>470500164.6</td>\n",
       "      <td>21.508145</td>\n",
       "      <td>22.002919</td>\n",
       "      <td>21.496503</td>\n",
       "      <td>21.967995</td>\n",
       "      <td>...</td>\n",
       "      <td>80.62768</td>\n",
       "      <td>127.12</td>\n",
       "      <td>211989.2</td>\n",
       "      <td>1510.4</td>\n",
       "      <td>6.16</td>\n",
       "      <td>0.083906</td>\n",
       "      <td>0.023322</td>\n",
       "      <td>0.002588</td>\n",
       "      <td>-0.009565</td>\n",
       "      <td>0.190687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11210</th>\n",
       "      <td>2004-08-17</td>\n",
       "      <td>8.640643</td>\n",
       "      <td>8.696619</td>\n",
       "      <td>8.567551</td>\n",
       "      <td>8.629448</td>\n",
       "      <td>213490024.8</td>\n",
       "      <td>22.090238</td>\n",
       "      <td>22.264864</td>\n",
       "      <td>22.037850</td>\n",
       "      <td>22.078596</td>\n",
       "      <td>...</td>\n",
       "      <td>79.86140</td>\n",
       "      <td>124.56</td>\n",
       "      <td>202446.4</td>\n",
       "      <td>1524.8</td>\n",
       "      <td>6.34</td>\n",
       "      <td>-0.161147</td>\n",
       "      <td>0.005022</td>\n",
       "      <td>-0.001478</td>\n",
       "      <td>0.053358</td>\n",
       "      <td>-0.308864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11211</th>\n",
       "      <td>2004-08-18</td>\n",
       "      <td>8.855044</td>\n",
       "      <td>8.939680</td>\n",
       "      <td>8.767921</td>\n",
       "      <td>8.838277</td>\n",
       "      <td>286163806.6</td>\n",
       "      <td>22.061139</td>\n",
       "      <td>22.334718</td>\n",
       "      <td>22.020393</td>\n",
       "      <td>22.305614</td>\n",
       "      <td>...</td>\n",
       "      <td>81.29254</td>\n",
       "      <td>127.44</td>\n",
       "      <td>214431.0</td>\n",
       "      <td>1578.6</td>\n",
       "      <td>6.02</td>\n",
       "      <td>0.023911</td>\n",
       "      <td>0.010230</td>\n",
       "      <td>0.015044</td>\n",
       "      <td>0.064421</td>\n",
       "      <td>-0.052083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11212</th>\n",
       "      <td>2024-09-06</td>\n",
       "      <td>115.948524</td>\n",
       "      <td>116.838441</td>\n",
       "      <td>114.974020</td>\n",
       "      <td>115.838497</td>\n",
       "      <td>16462160.0</td>\n",
       "      <td>217.600006</td>\n",
       "      <td>218.740005</td>\n",
       "      <td>211.089996</td>\n",
       "      <td>212.850006</td>\n",
       "      <td>...</td>\n",
       "      <td>94.36160</td>\n",
       "      <td>180.50</td>\n",
       "      <td>370425.2</td>\n",
       "      <td>1295.4</td>\n",
       "      <td>8.06</td>\n",
       "      <td>2.573105</td>\n",
       "      <td>2.255749</td>\n",
       "      <td>3.159481</td>\n",
       "      <td>7.051063</td>\n",
       "      <td>0.869934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date   GOOG_Open   GOOG_High    GOOG_Low  GOOG_Close  GOOG_Volume  \\\n",
       "11208 2004-08-13    9.351367    9.432421    9.264442    9.322409  247552422.0   \n",
       "11209 2004-08-16   10.168621   10.312218    9.998105   10.138369  470500164.6   \n",
       "11210 2004-08-17    8.640643    8.696619    8.567551    8.629448  213490024.8   \n",
       "11211 2004-08-18    8.855044    8.939680    8.767921    8.838277  286163806.6   \n",
       "11212 2024-09-06  115.948524  116.838441  114.974020  115.838497   16462160.0   \n",
       "\n",
       "         JPM_Open    JPM_High     JPM_Low   JPM_Close  ...    INDPRO  PPIACO  \\\n",
       "11208   21.432474   21.496504   21.246206   21.461578  ...  70.72818  121.60   \n",
       "11209   21.508145   22.002919   21.496503   21.967995  ...  80.62768  127.12   \n",
       "11210   22.090238   22.264864   22.037850   22.078596  ...  79.86140  124.56   \n",
       "11211   22.061139   22.334718   22.020393   22.305614  ...  81.29254  127.44   \n",
       "11212  217.600006  218.740005  211.089996  212.850006  ...  94.36160  180.50   \n",
       "\n",
       "          RSXFS   HOUST  PSAVERT  GOOG_LogReturn  JPM_LogReturn  \\\n",
       "11208  175757.4  1357.4     7.40       -1.281036       0.000271   \n",
       "11209  211989.2  1510.4     6.16        0.083906       0.023322   \n",
       "11210  202446.4  1524.8     6.34       -0.161147       0.005022   \n",
       "11211  214431.0  1578.6     6.02        0.023911       0.010230   \n",
       "11212  370425.2  1295.4     8.06        2.573105       2.255749   \n",
       "\n",
       "       MSFT_LogReturn  NVDA_LogReturn  VOO_LogReturn  \n",
       "11208        0.005195        0.040822      -0.319949  \n",
       "11209        0.002588       -0.009565       0.190687  \n",
       "11210       -0.001478        0.053358      -0.308864  \n",
       "11211        0.015044        0.064421      -0.052083  \n",
       "11212        3.159481        7.051063       0.869934  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df_imputed.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0ea9b9f2-14e7-4dd2-8a82-c2df32f64f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling Volatility for each stock (Relevant for identifying periods of high/low risk)\n",
    "for stock in stocks:\n",
    "    full_df_imputed[f'{stock}_RollingVolatility_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).std()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "084170c4-1cc9-4c84-9eef-db1ce0663be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Percentage changes in economic indicators (Capture macroeconomic shifts and trends, important for risk modeling)\n",
    "for indicator in economic_indicators:\n",
    "    full_df_imputed[f'{indicator}_PctChange'] = full_df_imputed[indicator].pct_change()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ba8b938-e08a-4716-ac00-0170ca605e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving averages of stock closing prices (Relevant for identifying trends and stock behavior over time)\n",
    "for stock in stocks:\n",
    "    full_df_imputed[f'{stock}_MA10'] = full_df_imputed[f'{stock}_Close'].rolling(window=10).mean()  # Short-term trend\n",
    "    full_df_imputed[f'{stock}_MA200'] = full_df_imputed[f'{stock}_Close'].rolling(window=200).mean()  # Long-term trend\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f44687e-5546-4659-a783-58b0e9288ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag features for log returns and economic indicators (Relevant for capturing past behavior and predicting future movements)\n",
    "lags = [1, 5, 10]\n",
    "for stock in stocks:\n",
    "    for lag in lags:\n",
    "        full_df_imputed[f'{stock}_LogReturn_Lag{lag}'] = full_df_imputed[f'{stock}_LogReturn'].shift(lag)\n",
    "\n",
    "for indicator in economic_indicators:\n",
    "    for lag in lags:\n",
    "        full_df_imputed[f'{indicator}_PctChange_Lag{lag}'] = full_df_imputed[f'{indicator}_PctChange'].shift(lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47ae4e17-6b3d-40db-be8e-bdd04ab9c42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum drawdown for each stock (Capture the largest loss from peak to trough, relevant for modeling worst-case scenarios)\n",
    "for stock in stocks:\n",
    "    full_df_imputed[f'{stock}_MaxDrawdown'] = full_df_imputed[f'{stock}_Close'].rolling(window=252, min_periods=1).apply(lambda x: (x.max() - x.min()) / x.max(), raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4aedcfc0-780f-4c3e-9d99-f2c6e55ba6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rolling correlation between stock log returns (Relevant for understanding stock relationships, which can impact portfolio risk)\n",
    "for i, stock1 in enumerate(stocks):\n",
    "    for stock2 in stocks[i+1:]:\n",
    "        full_df_imputed[f'{stock1}_{stock2}_RollingCorr_30d'] = full_df_imputed[f'{stock1}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{stock2}_LogReturn'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ee2b6821-a70f-4992-99ab-d9931e0311eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\1290757663.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n"
     ]
    }
   ],
   "source": [
    "# Rolling correlation between stock returns and economic indicators (Capture how economic factors affect stock movements)\n",
    "for stock in stocks:\n",
    "    for indicator in economic_indicators:\n",
    "        full_df_imputed[f'{stock}_{indicator}_RollingCorr_30d'] = full_df_imputed[f'{stock}_LogReturn'].rolling(window=30).corr(full_df_imputed[f'{indicator}_PctChange'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f50ee712-c909-404f-aa44-4e01061f26e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\839149101.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_Outliers'] = np.abs(full_df_imputed[f'{stock}_LogReturn']) > (full_df_imputed[f'{stock}_LogReturn'].mean() + 3 * full_df_imputed[f'{stock}_LogReturn'].std())\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\839149101.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_Outliers'] = np.abs(full_df_imputed[f'{stock}_LogReturn']) > (full_df_imputed[f'{stock}_LogReturn'].mean() + 3 * full_df_imputed[f'{stock}_LogReturn'].std())\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\839149101.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_Outliers'] = np.abs(full_df_imputed[f'{stock}_LogReturn']) > (full_df_imputed[f'{stock}_LogReturn'].mean() + 3 * full_df_imputed[f'{stock}_LogReturn'].std())\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\839149101.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_Outliers'] = np.abs(full_df_imputed[f'{stock}_LogReturn']) > (full_df_imputed[f'{stock}_LogReturn'].mean() + 3 * full_df_imputed[f'{stock}_LogReturn'].std())\n",
      "C:\\Users\\Rj\\AppData\\Local\\Temp\\ipykernel_6204\\839149101.py:3: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  full_df_imputed[f'{stock}_Outliers'] = np.abs(full_df_imputed[f'{stock}_LogReturn']) > (full_df_imputed[f'{stock}_LogReturn'].mean() + 3 * full_df_imputed[f'{stock}_LogReturn'].std())\n"
     ]
    }
   ],
   "source": [
    "# Outlier detection (Flagging large returns for risk identification, which may indicate unusual events or risks)\n",
    "for stock in stocks:\n",
    "    full_df_imputed[f'{stock}_Outliers'] = np.abs(full_df_imputed[f'{stock}_LogReturn']) > (full_df_imputed[f'{stock}_LogReturn'].mean() + 3 * full_df_imputed[f'{stock}_LogReturn'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c473d411-4470-497d-80af-86706d38a970",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date             0\n",
       "GOOG_Open        0\n",
       "GOOG_High        0\n",
       "GOOG_Low         0\n",
       "GOOG_Close       0\n",
       "                ..\n",
       "GOOG_Outliers    0\n",
       "JPM_Outliers     0\n",
       "MSFT_Outliers    0\n",
       "NVDA_Outliers    0\n",
       "VOO_Outliers     0\n",
       "Length: 182, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df_imputed.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "98eb3492-4d3a-4df5-9ea1-1e2275871b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 11213 entries, 0 to 11212\n",
      "Columns: 182 entries, Date to VOO_Outliers\n",
      "dtypes: bool(5), datetime64[ns](1), float64(175), int64(1)\n",
      "memory usage: 15.2 MB\n"
     ]
    }
   ],
   "source": [
    "full_df_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "71957b51-1aa0-46d4-937c-307baf38ab10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaN values caused by the shift and rolling operations\n",
    "full_df_imputed.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b4fe7c7-e8b0-4581-bb74-963524dde5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9603 entries, 199 to 11212\n",
      "Columns: 182 entries, Date to VOO_Outliers\n",
      "dtypes: bool(5), datetime64[ns](1), float64(175), int64(1)\n",
      "memory usage: 13.1 MB\n"
     ]
    }
   ],
   "source": [
    "full_df_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d527bb03-9896-4b94-8be2-bdf711d4eda5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'GOOG_Open', 'GOOG_High', 'GOOG_Low', 'GOOG_Close',\n",
       "       'GOOG_Volume', 'JPM_Open', 'JPM_High', 'JPM_Low', 'JPM_Close',\n",
       "       ...\n",
       "       'VOO_INDPRO_RollingCorr_30d', 'VOO_PPIACO_RollingCorr_30d',\n",
       "       'VOO_RSXFS_RollingCorr_30d', 'VOO_HOUST_RollingCorr_30d',\n",
       "       'VOO_PSAVERT_RollingCorr_30d', 'GOOG_Outliers', 'JPM_Outliers',\n",
       "       'MSFT_Outliers', 'NVDA_Outliers', 'VOO_Outliers'],\n",
       "      dtype='object', length=182)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df_imputed.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "815a48bb-576f-4f4d-86da-f8eeb41e152b",
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_columns = full_df_imputed.select_dtypes(include='bool').columns\n",
    "full_df_imputed[boolean_columns] = full_df_imputed[boolean_columns].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "60fa983b-3fd0-4ab0-b0c0-3db4449f7b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 9603 entries, 199 to 11212\n",
      "Columns: 182 entries, Date to VOO_Outliers\n",
      "dtypes: datetime64[ns](1), float64(175), int32(5), int64(1)\n",
      "memory usage: 13.2 MB\n"
     ]
    }
   ],
   "source": [
    "full_df_imputed.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c1a21e18-c61a-44ea-811d-7b4132c9d9d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>GOOG_Open</th>\n",
       "      <th>GOOG_High</th>\n",
       "      <th>GOOG_Low</th>\n",
       "      <th>GOOG_Close</th>\n",
       "      <th>GOOG_Volume</th>\n",
       "      <th>JPM_Open</th>\n",
       "      <th>JPM_High</th>\n",
       "      <th>JPM_Low</th>\n",
       "      <th>JPM_Close</th>\n",
       "      <th>...</th>\n",
       "      <th>VOO_INDPRO_RollingCorr_30d</th>\n",
       "      <th>VOO_PPIACO_RollingCorr_30d</th>\n",
       "      <th>VOO_RSXFS_RollingCorr_30d</th>\n",
       "      <th>VOO_HOUST_RollingCorr_30d</th>\n",
       "      <th>VOO_PSAVERT_RollingCorr_30d</th>\n",
       "      <th>GOOG_Outliers</th>\n",
       "      <th>JPM_Outliers</th>\n",
       "      <th>MSFT_Outliers</th>\n",
       "      <th>NVDA_Outliers</th>\n",
       "      <th>VOO_Outliers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>2005-06-03</td>\n",
       "      <td>7.134857</td>\n",
       "      <td>7.197302</td>\n",
       "      <td>6.901498</td>\n",
       "      <td>6.972402</td>\n",
       "      <td>7.541080e+08</td>\n",
       "      <td>21.304895</td>\n",
       "      <td>21.442462</td>\n",
       "      <td>21.179291</td>\n",
       "      <td>21.263027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.166449</td>\n",
       "      <td>0.044148</td>\n",
       "      <td>0.132329</td>\n",
       "      <td>0.029748</td>\n",
       "      <td>-0.113528</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>2005-06-06</td>\n",
       "      <td>7.025393</td>\n",
       "      <td>7.308011</td>\n",
       "      <td>7.011461</td>\n",
       "      <td>7.238102</td>\n",
       "      <td>9.044133e+08</td>\n",
       "      <td>21.215175</td>\n",
       "      <td>21.286948</td>\n",
       "      <td>21.053683</td>\n",
       "      <td>21.239098</td>\n",
       "      <td>...</td>\n",
       "      <td>0.162426</td>\n",
       "      <td>0.042463</td>\n",
       "      <td>0.127908</td>\n",
       "      <td>0.026479</td>\n",
       "      <td>-0.110943</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>2005-06-07</td>\n",
       "      <td>7.391353</td>\n",
       "      <td>7.453299</td>\n",
       "      <td>7.222181</td>\n",
       "      <td>7.292336</td>\n",
       "      <td>9.765667e+08</td>\n",
       "      <td>21.245088</td>\n",
       "      <td>21.496296</td>\n",
       "      <td>21.203220</td>\n",
       "      <td>21.221163</td>\n",
       "      <td>...</td>\n",
       "      <td>0.044864</td>\n",
       "      <td>-0.072539</td>\n",
       "      <td>-0.011971</td>\n",
       "      <td>-0.099561</td>\n",
       "      <td>-0.033519</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>2005-06-08</td>\n",
       "      <td>7.285620</td>\n",
       "      <td>7.294079</td>\n",
       "      <td>6.916177</td>\n",
       "      <td>6.954987</td>\n",
       "      <td>1.031889e+09</td>\n",
       "      <td>21.263029</td>\n",
       "      <td>21.418541</td>\n",
       "      <td>21.221161</td>\n",
       "      <td>21.334803</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.042881</td>\n",
       "      <td>-0.160947</td>\n",
       "      <td>-0.103852</td>\n",
       "      <td>-0.179657</td>\n",
       "      <td>0.038537</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>2005-06-09</td>\n",
       "      <td>7.083359</td>\n",
       "      <td>7.177399</td>\n",
       "      <td>6.979865</td>\n",
       "      <td>7.122915</td>\n",
       "      <td>6.601090e+08</td>\n",
       "      <td>21.245086</td>\n",
       "      <td>21.352747</td>\n",
       "      <td>21.119483</td>\n",
       "      <td>21.233124</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.162425</td>\n",
       "      <td>-0.250860</td>\n",
       "      <td>-0.216888</td>\n",
       "      <td>-0.283785</td>\n",
       "      <td>0.150009</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11208</th>\n",
       "      <td>2004-08-13</td>\n",
       "      <td>9.351367</td>\n",
       "      <td>9.432421</td>\n",
       "      <td>9.264442</td>\n",
       "      <td>9.322409</td>\n",
       "      <td>2.475524e+08</td>\n",
       "      <td>21.432474</td>\n",
       "      <td>21.496504</td>\n",
       "      <td>21.246206</td>\n",
       "      <td>21.461578</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.447463</td>\n",
       "      <td>-0.435496</td>\n",
       "      <td>-0.475453</td>\n",
       "      <td>-0.078723</td>\n",
       "      <td>0.225041</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11209</th>\n",
       "      <td>2004-08-16</td>\n",
       "      <td>10.168621</td>\n",
       "      <td>10.312218</td>\n",
       "      <td>9.998105</td>\n",
       "      <td>10.138369</td>\n",
       "      <td>4.705002e+08</td>\n",
       "      <td>21.508145</td>\n",
       "      <td>22.002919</td>\n",
       "      <td>21.496503</td>\n",
       "      <td>21.967995</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.337156</td>\n",
       "      <td>-0.378505</td>\n",
       "      <td>-0.375912</td>\n",
       "      <td>0.064686</td>\n",
       "      <td>0.121821</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11210</th>\n",
       "      <td>2004-08-17</td>\n",
       "      <td>8.640643</td>\n",
       "      <td>8.696619</td>\n",
       "      <td>8.567551</td>\n",
       "      <td>8.629448</td>\n",
       "      <td>2.134900e+08</td>\n",
       "      <td>22.090238</td>\n",
       "      <td>22.264864</td>\n",
       "      <td>22.037850</td>\n",
       "      <td>22.078596</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.333802</td>\n",
       "      <td>-0.367000</td>\n",
       "      <td>-0.358211</td>\n",
       "      <td>0.070966</td>\n",
       "      <td>0.118865</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11211</th>\n",
       "      <td>2004-08-18</td>\n",
       "      <td>8.855044</td>\n",
       "      <td>8.939680</td>\n",
       "      <td>8.767921</td>\n",
       "      <td>8.838277</td>\n",
       "      <td>2.861638e+08</td>\n",
       "      <td>22.061139</td>\n",
       "      <td>22.334718</td>\n",
       "      <td>22.020393</td>\n",
       "      <td>22.305614</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.359951</td>\n",
       "      <td>-0.330508</td>\n",
       "      <td>-0.367405</td>\n",
       "      <td>-0.041417</td>\n",
       "      <td>0.149757</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11212</th>\n",
       "      <td>2024-09-06</td>\n",
       "      <td>115.948524</td>\n",
       "      <td>116.838441</td>\n",
       "      <td>114.974020</td>\n",
       "      <td>115.838497</td>\n",
       "      <td>1.646216e+07</td>\n",
       "      <td>217.600006</td>\n",
       "      <td>218.740005</td>\n",
       "      <td>211.089996</td>\n",
       "      <td>212.850006</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.143735</td>\n",
       "      <td>0.190709</td>\n",
       "      <td>0.096190</td>\n",
       "      <td>-0.208661</td>\n",
       "      <td>0.267911</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9603 rows × 182 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date   GOOG_Open   GOOG_High    GOOG_Low  GOOG_Close  \\\n",
       "199   2005-06-03    7.134857    7.197302    6.901498    6.972402   \n",
       "200   2005-06-06    7.025393    7.308011    7.011461    7.238102   \n",
       "201   2005-06-07    7.391353    7.453299    7.222181    7.292336   \n",
       "202   2005-06-08    7.285620    7.294079    6.916177    6.954987   \n",
       "203   2005-06-09    7.083359    7.177399    6.979865    7.122915   \n",
       "...          ...         ...         ...         ...         ...   \n",
       "11208 2004-08-13    9.351367    9.432421    9.264442    9.322409   \n",
       "11209 2004-08-16   10.168621   10.312218    9.998105   10.138369   \n",
       "11210 2004-08-17    8.640643    8.696619    8.567551    8.629448   \n",
       "11211 2004-08-18    8.855044    8.939680    8.767921    8.838277   \n",
       "11212 2024-09-06  115.948524  116.838441  114.974020  115.838497   \n",
       "\n",
       "        GOOG_Volume    JPM_Open    JPM_High     JPM_Low   JPM_Close  ...  \\\n",
       "199    7.541080e+08   21.304895   21.442462   21.179291   21.263027  ...   \n",
       "200    9.044133e+08   21.215175   21.286948   21.053683   21.239098  ...   \n",
       "201    9.765667e+08   21.245088   21.496296   21.203220   21.221163  ...   \n",
       "202    1.031889e+09   21.263029   21.418541   21.221161   21.334803  ...   \n",
       "203    6.601090e+08   21.245086   21.352747   21.119483   21.233124  ...   \n",
       "...             ...         ...         ...         ...         ...  ...   \n",
       "11208  2.475524e+08   21.432474   21.496504   21.246206   21.461578  ...   \n",
       "11209  4.705002e+08   21.508145   22.002919   21.496503   21.967995  ...   \n",
       "11210  2.134900e+08   22.090238   22.264864   22.037850   22.078596  ...   \n",
       "11211  2.861638e+08   22.061139   22.334718   22.020393   22.305614  ...   \n",
       "11212  1.646216e+07  217.600006  218.740005  211.089996  212.850006  ...   \n",
       "\n",
       "       VOO_INDPRO_RollingCorr_30d  VOO_PPIACO_RollingCorr_30d  \\\n",
       "199                      0.166449                    0.044148   \n",
       "200                      0.162426                    0.042463   \n",
       "201                      0.044864                   -0.072539   \n",
       "202                     -0.042881                   -0.160947   \n",
       "203                     -0.162425                   -0.250860   \n",
       "...                           ...                         ...   \n",
       "11208                   -0.447463                   -0.435496   \n",
       "11209                   -0.337156                   -0.378505   \n",
       "11210                   -0.333802                   -0.367000   \n",
       "11211                   -0.359951                   -0.330508   \n",
       "11212                   -0.143735                    0.190709   \n",
       "\n",
       "       VOO_RSXFS_RollingCorr_30d  VOO_HOUST_RollingCorr_30d  \\\n",
       "199                     0.132329                   0.029748   \n",
       "200                     0.127908                   0.026479   \n",
       "201                    -0.011971                  -0.099561   \n",
       "202                    -0.103852                  -0.179657   \n",
       "203                    -0.216888                  -0.283785   \n",
       "...                          ...                        ...   \n",
       "11208                  -0.475453                  -0.078723   \n",
       "11209                  -0.375912                   0.064686   \n",
       "11210                  -0.358211                   0.070966   \n",
       "11211                  -0.367405                  -0.041417   \n",
       "11212                   0.096190                  -0.208661   \n",
       "\n",
       "       VOO_PSAVERT_RollingCorr_30d  GOOG_Outliers  JPM_Outliers  \\\n",
       "199                      -0.113528              0             0   \n",
       "200                      -0.110943              0             0   \n",
       "201                      -0.033519              0             0   \n",
       "202                       0.038537              0             0   \n",
       "203                       0.150009              0             0   \n",
       "...                            ...            ...           ...   \n",
       "11208                     0.225041              0             0   \n",
       "11209                     0.121821              0             0   \n",
       "11210                     0.118865              0             0   \n",
       "11211                     0.149757              0             0   \n",
       "11212                     0.267911              1             1   \n",
       "\n",
       "       MSFT_Outliers  NVDA_Outliers  VOO_Outliers  \n",
       "199                0              0             0  \n",
       "200                0              0             0  \n",
       "201                0              0             0  \n",
       "202                0              0             0  \n",
       "203                0              0             0  \n",
       "...              ...            ...           ...  \n",
       "11208              0              0             0  \n",
       "11209              0              0             0  \n",
       "11210              0              0             0  \n",
       "11211              0              0             0  \n",
       "11212              1              1             1  \n",
       "\n",
       "[9603 rows x 182 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df_imputed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1980d754-2d64-4db4-b659-a4d764f83b5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0612fe-a197-412e-af24-9038a031e8bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a9ea3c-bf38-4888-87a7-5f4554f622e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36880791-2a3f-4660-adcc-01347ffab057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f9ba77-5bbb-4b62-a0c1-01f991c61881",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa18c33f-b7ba-4c77-a832-b2090b5a2549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdf9bad-7a66-4ed9-a20c-b02cd73e31c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cf4626-e970-438b-aafd-fe4c6f28d7c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "19276d0c-c991-498e-b1d5-211cc48c0fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Select relevant features (excluding 'Date')\n",
    "features = full_df_imputed.columns.difference(['Date'])\n",
    "X = full_df_imputed[features]\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Create a DataFrame with scaled features\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f0472349-443a-458a-8882-664ac195ceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_sequences(X, y, time_steps=1):\n",
    "    Xs, ys = [], []\n",
    "    for i in range(len(X) - time_steps):\n",
    "        Xs.append(X.iloc[i:(i + time_steps)].values)\n",
    "        ys.append(y.iloc[i + time_steps].values)\n",
    "    return np.array(Xs), np.array(ys)\n",
    "\n",
    "time_steps = 1\n",
    "closing_prices = full_df_imputed[[col for col in full_df_imputed.columns if 'Close' in col]]\n",
    "X_sequences, y_sequences = create_sequences(X_scaled_df, closing_prices, time_steps)\n",
    "\n",
    "# Split into training and testing sets\n",
    "train_size = int(len(X_sequences) * 0.8)\n",
    "X_train, X_test = X_sequences[:train_size], X_sequences[train_size:]\n",
    "y_train, y_test = y_sequences[:train_size], y_sequences[train_size:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f7be6664-c957-4142-bb07-e5efcccc9cb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7681, 1, 181)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "87ef2a41-4772-4cfe-adce-7bfa5bb2d281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7681, 5)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "raw",
   "id": "10a373ff-61ec-42a8-8ab0-f5af17306e61",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "07eebd8f-a229-4ebd-875e-33c8291fdc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Personal\\Risk_management_model\\.venv\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(time_steps, X_train.shape[2])))\n",
    "model.add(LSTM(50, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(25))\n",
    "model.add(Dense(y_train.shape[1]))  # Output layer for multiple features (closing prices of all tickers)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "564a48f0-710f-4e1b-a25e-6c11beceffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='nadam', loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "03a3e57c-93e5-4faf-b9ea-db77d0c7957b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.0159 - val_loss: 1963.2292\n",
      "Epoch 2/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72.1463 - val_loss: 1965.2006\n",
      "Epoch 3/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.8181 - val_loss: 1928.1709\n",
      "Epoch 4/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.2944 - val_loss: 1940.2765\n",
      "Epoch 5/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76.9382 - val_loss: 1930.6760\n",
      "Epoch 6/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.9467 - val_loss: 1949.2273\n",
      "Epoch 7/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 79.4062 - val_loss: 1961.8435\n",
      "Epoch 8/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.3988 - val_loss: 1927.9376\n",
      "Epoch 9/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 79.1001 - val_loss: 1913.5310\n",
      "Epoch 10/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.7291 - val_loss: 1945.6658\n",
      "Epoch 11/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.0304 - val_loss: 1921.6453\n",
      "Epoch 12/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.2844 - val_loss: 1928.6957\n",
      "Epoch 13/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.6378 - val_loss: 1946.1310\n",
      "Epoch 14/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76.6055 - val_loss: 1917.8041\n",
      "Epoch 15/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 77.9768 - val_loss: 1960.0800\n",
      "Epoch 16/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72.4887 - val_loss: 1964.0828\n",
      "Epoch 17/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72.7222 - val_loss: 1962.8334\n",
      "Epoch 18/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 77.5024 - val_loss: 1949.3982\n",
      "Epoch 19/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76.3836 - val_loss: 1951.5515\n",
      "Epoch 20/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.6059 - val_loss: 1963.1658\n",
      "Epoch 21/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.7888 - val_loss: 1958.6417\n",
      "Epoch 22/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76.7097 - val_loss: 1928.1521\n",
      "Epoch 23/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.6618 - val_loss: 1957.4598\n",
      "Epoch 24/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.1938 - val_loss: 1948.5806\n",
      "Epoch 25/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.4883 - val_loss: 1966.8221\n",
      "Epoch 26/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 82.0814 - val_loss: 1944.2015\n",
      "Epoch 27/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.7270 - val_loss: 1943.4955\n",
      "Epoch 28/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.2596 - val_loss: 1960.0555\n",
      "Epoch 29/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 77.4611 - val_loss: 1955.4124\n",
      "Epoch 30/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.4005 - val_loss: 1956.5024\n",
      "Epoch 31/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.4907 - val_loss: 1956.8922\n",
      "Epoch 32/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76.1395 - val_loss: 1958.8038\n",
      "Epoch 33/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.9125 - val_loss: 1909.3097\n",
      "Epoch 34/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72.1182 - val_loss: 1938.5725\n",
      "Epoch 35/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 77.7749 - val_loss: 1995.9489\n",
      "Epoch 36/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 80.0026 - val_loss: 1945.6323\n",
      "Epoch 37/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.7965 - val_loss: 1956.5872\n",
      "Epoch 38/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.0698 - val_loss: 1932.2656\n",
      "Epoch 39/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.9694 - val_loss: 1966.4146\n",
      "Epoch 40/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.4314 - val_loss: 1952.9441\n",
      "Epoch 41/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 79.1739 - val_loss: 1957.9901\n",
      "Epoch 42/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 81.3194 - val_loss: 1955.9720\n",
      "Epoch 43/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 78.4869 - val_loss: 1928.6671\n",
      "Epoch 44/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.0096 - val_loss: 1958.9352\n",
      "Epoch 45/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.4603 - val_loss: 1945.5957\n",
      "Epoch 46/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76.0630 - val_loss: 1979.4867\n",
      "Epoch 47/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.2016 - val_loss: 1978.6028\n",
      "Epoch 48/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.0645 - val_loss: 1949.2834\n",
      "Epoch 49/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 71.8908 - val_loss: 1921.3632\n",
      "Epoch 50/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72.9415 - val_loss: 1944.4329\n",
      "Epoch 51/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 82.2217 - val_loss: 1998.6172\n",
      "Epoch 52/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 71.8266 - val_loss: 1965.6268\n",
      "Epoch 53/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.3122 - val_loss: 1976.0801\n",
      "Epoch 54/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.7384 - val_loss: 1952.1954\n",
      "Epoch 55/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.7920 - val_loss: 2004.7485\n",
      "Epoch 56/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72.5818 - val_loss: 1959.1362\n",
      "Epoch 57/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72.9590 - val_loss: 1900.5347\n",
      "Epoch 58/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72.0882 - val_loss: 1941.8636\n",
      "Epoch 59/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.2372 - val_loss: 1972.3230\n",
      "Epoch 60/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76.9760 - val_loss: 1945.6840\n",
      "Epoch 61/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.9355 - val_loss: 1949.4849\n",
      "Epoch 62/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.4009 - val_loss: 1959.8516\n",
      "Epoch 63/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72.6385 - val_loss: 1984.3744\n",
      "Epoch 64/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 77.0955 - val_loss: 1947.6112\n",
      "Epoch 65/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.2283 - val_loss: 1934.0304\n",
      "Epoch 66/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 78.6892 - val_loss: 1904.7683\n",
      "Epoch 67/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76.5858 - val_loss: 1974.0974\n",
      "Epoch 68/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 77.6570 - val_loss: 1955.6971\n",
      "Epoch 69/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76.9010 - val_loss: 1909.9618\n",
      "Epoch 70/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72.8152 - val_loss: 1907.2426\n",
      "Epoch 71/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.2892 - val_loss: 1925.1711\n",
      "Epoch 72/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76.3788 - val_loss: 1922.1122\n",
      "Epoch 73/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.9856 - val_loss: 1950.4106\n",
      "Epoch 74/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 78.1359 - val_loss: 1969.8688\n",
      "Epoch 75/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76.8232 - val_loss: 1954.1869\n",
      "Epoch 76/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.6991 - val_loss: 1939.1520\n",
      "Epoch 77/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 77.6406 - val_loss: 1943.3329\n",
      "Epoch 78/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 77.0196 - val_loss: 1945.9568\n",
      "Epoch 79/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.5560 - val_loss: 1935.9570\n",
      "Epoch 80/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72.7729 - val_loss: 1980.8479\n",
      "Epoch 81/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.0638 - val_loss: 1984.8773\n",
      "Epoch 82/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.6337 - val_loss: 1995.4768\n",
      "Epoch 83/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 77.8549 - val_loss: 1942.7635\n",
      "Epoch 84/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.8972 - val_loss: 1965.7898\n",
      "Epoch 85/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.6195 - val_loss: 1968.8523\n",
      "Epoch 86/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 76.5947 - val_loss: 1923.4484\n",
      "Epoch 87/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.1772 - val_loss: 1988.8308\n",
      "Epoch 88/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.8714 - val_loss: 1951.9189\n",
      "Epoch 89/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.4837 - val_loss: 1972.7043\n",
      "Epoch 90/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72.0675 - val_loss: 1967.8099\n",
      "Epoch 91/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.2223 - val_loss: 2003.6110\n",
      "Epoch 92/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76.3194 - val_loss: 1946.3181\n",
      "Epoch 93/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76.4806 - val_loss: 1968.2324\n",
      "Epoch 94/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.9794 - val_loss: 1933.7961\n",
      "Epoch 95/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 77.5441 - val_loss: 1990.1367\n",
      "Epoch 96/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.1205 - val_loss: 1992.9315\n",
      "Epoch 97/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 71.0631 - val_loss: 1964.9474\n",
      "Epoch 98/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 79.4720 - val_loss: 1938.1149\n",
      "Epoch 99/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.4206 - val_loss: 1970.4739\n",
      "Epoch 100/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.4733 - val_loss: 1959.7609\n",
      "Epoch 101/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.9317 - val_loss: 1959.6998\n",
      "Epoch 102/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72.4388 - val_loss: 1958.3308\n",
      "Epoch 103/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.7158 - val_loss: 1953.1876\n",
      "Epoch 104/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.7832 - val_loss: 1941.2815\n",
      "Epoch 105/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.7820 - val_loss: 1965.0726\n",
      "Epoch 106/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.6108 - val_loss: 1989.6250\n",
      "Epoch 107/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.3955 - val_loss: 1976.5476\n",
      "Epoch 108/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76.7207 - val_loss: 1958.0354\n",
      "Epoch 109/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72.7263 - val_loss: 1948.9559\n",
      "Epoch 110/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 68.1835 - val_loss: 1952.8862\n",
      "Epoch 111/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.3835 - val_loss: 1966.6848\n",
      "Epoch 112/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.5009 - val_loss: 1982.5137\n",
      "Epoch 113/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.0374 - val_loss: 1965.1229\n",
      "Epoch 114/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 77.8688 - val_loss: 1946.8472\n",
      "Epoch 115/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76.6285 - val_loss: 1945.8646\n",
      "Epoch 116/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 77.5657 - val_loss: 1989.7030\n",
      "Epoch 117/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.4675 - val_loss: 1948.5500\n",
      "Epoch 118/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 75.7767 - val_loss: 1903.7626\n",
      "Epoch 119/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.9580 - val_loss: 1948.2407\n",
      "Epoch 120/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.3298 - val_loss: 1939.3353\n",
      "Epoch 121/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 78.8509 - val_loss: 1999.6908\n",
      "Epoch 122/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72.2739 - val_loss: 1934.2971\n",
      "Epoch 123/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.8743 - val_loss: 1968.0435\n",
      "Epoch 124/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.7030 - val_loss: 1946.4908\n",
      "Epoch 125/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.3344 - val_loss: 1941.5690\n",
      "Epoch 126/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.7474 - val_loss: 1956.3741\n",
      "Epoch 127/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 71.9825 - val_loss: 2001.9750\n",
      "Epoch 128/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.3157 - val_loss: 1980.5638\n",
      "Epoch 129/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.3530 - val_loss: 1930.5016\n",
      "Epoch 130/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.5528 - val_loss: 1967.4006\n",
      "Epoch 131/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72.9000 - val_loss: 1951.6774\n",
      "Epoch 132/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 80.9361 - val_loss: 1943.5376\n",
      "Epoch 133/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 77.0029 - val_loss: 1970.2063\n",
      "Epoch 134/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76.8063 - val_loss: 1974.4464\n",
      "Epoch 135/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.6332 - val_loss: 1968.6888\n",
      "Epoch 136/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.8483 - val_loss: 2034.6940\n",
      "Epoch 137/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 77.0027 - val_loss: 1965.1364\n",
      "Epoch 138/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 78.4632 - val_loss: 1974.0775\n",
      "Epoch 139/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76.7872 - val_loss: 1961.8903\n",
      "Epoch 140/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.9547 - val_loss: 1919.8785\n",
      "Epoch 141/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.7681 - val_loss: 2013.4094\n",
      "Epoch 142/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 71.1894 - val_loss: 1957.6863\n",
      "Epoch 143/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76.3463 - val_loss: 1939.8906\n",
      "Epoch 144/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 71.9290 - val_loss: 1948.4019\n",
      "Epoch 145/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.2465 - val_loss: 1961.2990\n",
      "Epoch 146/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72.2642 - val_loss: 2006.6237\n",
      "Epoch 147/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.8106 - val_loss: 2005.3040\n",
      "Epoch 148/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.8064 - val_loss: 1996.8000\n",
      "Epoch 149/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 71.1773 - val_loss: 1989.1073\n",
      "Epoch 150/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.4110 - val_loss: 1998.8004\n",
      "Epoch 151/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 77.3719 - val_loss: 1985.4342\n",
      "Epoch 152/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 71.8093 - val_loss: 1951.3719\n",
      "Epoch 153/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76.3978 - val_loss: 1971.9832\n",
      "Epoch 154/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.0116 - val_loss: 2033.7314\n",
      "Epoch 155/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76.4259 - val_loss: 1973.3467\n",
      "Epoch 156/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 70.8208 - val_loss: 1933.9519\n",
      "Epoch 157/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.0107 - val_loss: 2002.4901\n",
      "Epoch 158/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 77.9329 - val_loss: 1968.7742\n",
      "Epoch 159/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.4234 - val_loss: 1978.2235\n",
      "Epoch 160/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.1872 - val_loss: 1989.2328\n",
      "Epoch 161/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 78.1061 - val_loss: 1980.8074\n",
      "Epoch 162/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.9944 - val_loss: 1965.0585\n",
      "Epoch 163/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72.1459 - val_loss: 2021.5852\n",
      "Epoch 164/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76.1643 - val_loss: 1992.3168\n",
      "Epoch 165/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 70.6952 - val_loss: 1952.4242\n",
      "Epoch 166/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.0337 - val_loss: 1987.8842\n",
      "Epoch 167/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.0366 - val_loss: 1996.3123\n",
      "Epoch 168/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76.9373 - val_loss: 1937.1884\n",
      "Epoch 169/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.4488 - val_loss: 1974.1658\n",
      "Epoch 170/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72.0578 - val_loss: 1929.0699\n",
      "Epoch 171/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 77.7185 - val_loss: 1987.2847\n",
      "Epoch 172/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72.8734 - val_loss: 1994.1926\n",
      "Epoch 173/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.9823 - val_loss: 1973.0803\n",
      "Epoch 174/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.4341 - val_loss: 1991.1555\n",
      "Epoch 175/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 76.6211 - val_loss: 2003.3112\n",
      "Epoch 176/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.7070 - val_loss: 1979.4330\n",
      "Epoch 177/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.9259 - val_loss: 1986.6084\n",
      "Epoch 178/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.1841 - val_loss: 1970.9137\n",
      "Epoch 179/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.1003 - val_loss: 2003.4053\n",
      "Epoch 180/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.2320 - val_loss: 1951.0461\n",
      "Epoch 181/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 69.1794 - val_loss: 1982.5560\n",
      "Epoch 182/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72.0250 - val_loss: 1974.2567\n",
      "Epoch 183/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.2102 - val_loss: 2010.5116\n",
      "Epoch 184/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72.5366 - val_loss: 1991.4072\n",
      "Epoch 185/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.3968 - val_loss: 1974.3905\n",
      "Epoch 186/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 81.7295 - val_loss: 1954.9937\n",
      "Epoch 187/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 71.2487 - val_loss: 1984.3040\n",
      "Epoch 188/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 77.3088 - val_loss: 1993.9379\n",
      "Epoch 189/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.6784 - val_loss: 1951.3030\n",
      "Epoch 190/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76.0922 - val_loss: 1961.9280\n",
      "Epoch 191/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 70.7398 - val_loss: 1940.8613\n",
      "Epoch 192/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72.3036 - val_loss: 1988.1871\n",
      "Epoch 193/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.6519 - val_loss: 2008.9794\n",
      "Epoch 194/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.4753 - val_loss: 1951.6312\n",
      "Epoch 195/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.2945 - val_loss: 2005.9946\n",
      "Epoch 196/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.3312 - val_loss: 1951.1562\n",
      "Epoch 197/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.9555 - val_loss: 1997.3842\n",
      "Epoch 198/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72.4807 - val_loss: 1991.4877\n",
      "Epoch 199/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.6780 - val_loss: 1966.5652\n",
      "Epoch 200/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.8999 - val_loss: 1995.6133\n",
      "Epoch 201/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.5719 - val_loss: 1967.7196\n",
      "Epoch 202/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 78.3821 - val_loss: 1968.6951\n",
      "Epoch 203/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.8480 - val_loss: 1973.1624\n",
      "Epoch 204/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.8211 - val_loss: 1991.3842\n",
      "Epoch 205/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 69.9722 - val_loss: 1989.1771\n",
      "Epoch 206/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 76.0661 - val_loss: 1979.3636\n",
      "Epoch 207/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 81.5705 - val_loss: 1993.5574\n",
      "Epoch 208/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.2122 - val_loss: 1970.2045\n",
      "Epoch 209/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.7972 - val_loss: 1972.3423\n",
      "Epoch 210/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 71.2017 - val_loss: 1990.4829\n",
      "Epoch 211/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72.5464 - val_loss: 1951.4701\n",
      "Epoch 212/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.9546 - val_loss: 1939.7168\n",
      "Epoch 213/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.3211 - val_loss: 1948.9675\n",
      "Epoch 214/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.0230 - val_loss: 1997.7786\n",
      "Epoch 215/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76.7880 - val_loss: 1968.2965\n",
      "Epoch 216/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.9157 - val_loss: 1951.6833\n",
      "Epoch 217/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.9641 - val_loss: 1970.1003\n",
      "Epoch 218/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 78.9280 - val_loss: 1977.5778\n",
      "Epoch 219/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.2092 - val_loss: 1978.7485\n",
      "Epoch 220/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.1687 - val_loss: 1944.5371\n",
      "Epoch 221/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.1715 - val_loss: 1969.2513\n",
      "Epoch 222/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.6042 - val_loss: 1960.7157\n",
      "Epoch 223/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76.4433 - val_loss: 1976.0078\n",
      "Epoch 224/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.5322 - val_loss: 1959.7305\n",
      "Epoch 225/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.8536 - val_loss: 1941.6079\n",
      "Epoch 226/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 77.9786 - val_loss: 1952.9535\n",
      "Epoch 227/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.4442 - val_loss: 1946.2268\n",
      "Epoch 228/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.0118 - val_loss: 1947.1229\n",
      "Epoch 229/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.9421 - val_loss: 1945.3516\n",
      "Epoch 230/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.9582 - val_loss: 1955.3486\n",
      "Epoch 231/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.6635 - val_loss: 1982.6455\n",
      "Epoch 232/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 77.1494 - val_loss: 1985.9376\n",
      "Epoch 233/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 77.5768 - val_loss: 1955.7761\n",
      "Epoch 234/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72.2586 - val_loss: 1983.4099\n",
      "Epoch 235/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.2717 - val_loss: 1999.8794\n",
      "Epoch 236/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.1526 - val_loss: 1928.5614\n",
      "Epoch 237/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 76.1554 - val_loss: 1960.5593\n",
      "Epoch 238/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.5562 - val_loss: 1959.5981\n",
      "Epoch 239/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72.9228 - val_loss: 1973.0304\n",
      "Epoch 240/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.4741 - val_loss: 1959.5138\n",
      "Epoch 241/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.9408 - val_loss: 1972.2975\n",
      "Epoch 242/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72.5776 - val_loss: 1968.6494\n",
      "Epoch 243/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 77.0239 - val_loss: 1960.8617\n",
      "Epoch 244/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.1145 - val_loss: 1960.5377\n",
      "Epoch 245/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 71.0905 - val_loss: 1959.7369\n",
      "Epoch 246/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 79.9855 - val_loss: 1995.0864\n",
      "Epoch 247/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 70.7627 - val_loss: 1931.9598\n",
      "Epoch 248/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.0464 - val_loss: 1973.7333\n",
      "Epoch 249/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.8799 - val_loss: 1966.4476\n",
      "Epoch 250/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 70.7558 - val_loss: 1925.9254\n",
      "Epoch 251/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.6632 - val_loss: 1988.5505\n",
      "Epoch 252/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72.7297 - val_loss: 1978.6346\n",
      "Epoch 253/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.7162 - val_loss: 1952.1854\n",
      "Epoch 254/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.9878 - val_loss: 2015.1783\n",
      "Epoch 255/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.2838 - val_loss: 1966.5765\n",
      "Epoch 256/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.6723 - val_loss: 1966.0541\n",
      "Epoch 257/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 77.3587 - val_loss: 1958.5945\n",
      "Epoch 258/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.3151 - val_loss: 1969.4702\n",
      "Epoch 259/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.7637 - val_loss: 1945.5577\n",
      "Epoch 260/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.9228 - val_loss: 1934.5083\n",
      "Epoch 261/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.5891 - val_loss: 1976.2814\n",
      "Epoch 262/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.9838 - val_loss: 1968.0403\n",
      "Epoch 263/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72.7369 - val_loss: 1994.0303\n",
      "Epoch 264/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 76.4539 - val_loss: 1953.4878\n",
      "Epoch 265/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72.4399 - val_loss: 1951.4629\n",
      "Epoch 266/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 71.6113 - val_loss: 1946.4792\n",
      "Epoch 267/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.4058 - val_loss: 1935.7701\n",
      "Epoch 268/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 69.5550 - val_loss: 1961.6105\n",
      "Epoch 269/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.3576 - val_loss: 1932.1471\n",
      "Epoch 270/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72.5417 - val_loss: 1975.8391\n",
      "Epoch 271/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72.8610 - val_loss: 1967.3771\n",
      "Epoch 272/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 76.2326 - val_loss: 1980.5803\n",
      "Epoch 273/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 80.1143 - val_loss: 1962.0739\n",
      "Epoch 274/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 78.0070 - val_loss: 1943.9113\n",
      "Epoch 275/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72.7588 - val_loss: 1959.1965\n",
      "Epoch 276/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.5461 - val_loss: 2006.1421\n",
      "Epoch 277/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.1367 - val_loss: 2015.6083\n",
      "Epoch 278/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 80.1884 - val_loss: 2016.3347\n",
      "Epoch 279/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.3049 - val_loss: 1978.2495\n",
      "Epoch 280/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.1254 - val_loss: 1978.9535\n",
      "Epoch 281/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 77.5844 - val_loss: 2020.0216\n",
      "Epoch 282/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.1483 - val_loss: 1977.0004\n",
      "Epoch 283/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.1658 - val_loss: 2012.5829\n",
      "Epoch 284/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.5986 - val_loss: 1997.7137\n",
      "Epoch 285/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.1625 - val_loss: 1978.3763\n",
      "Epoch 286/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.1702 - val_loss: 1971.3717\n",
      "Epoch 287/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 79.2886 - val_loss: 2003.3629\n",
      "Epoch 288/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.0767 - val_loss: 1994.9457\n",
      "Epoch 289/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.7531 - val_loss: 2007.1447\n",
      "Epoch 290/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72.0495 - val_loss: 1979.5464\n",
      "Epoch 291/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.3278 - val_loss: 1975.7156\n",
      "Epoch 292/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.7258 - val_loss: 1940.7827\n",
      "Epoch 293/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.8452 - val_loss: 1944.0011\n",
      "Epoch 294/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 76.2426 - val_loss: 1976.5327\n",
      "Epoch 295/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76.5023 - val_loss: 2000.7936\n",
      "Epoch 296/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.8861 - val_loss: 2012.6747\n",
      "Epoch 297/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.6355 - val_loss: 1971.3157\n",
      "Epoch 298/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.0870 - val_loss: 1924.2761\n",
      "Epoch 299/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.3893 - val_loss: 1994.8063\n",
      "Epoch 300/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.5489 - val_loss: 1982.1985\n",
      "Epoch 301/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76.1415 - val_loss: 1999.0203\n",
      "Epoch 302/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.1543 - val_loss: 1986.2045\n",
      "Epoch 303/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.4427 - val_loss: 1979.8877\n",
      "Epoch 304/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.4985 - val_loss: 2014.8291\n",
      "Epoch 305/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.7893 - val_loss: 1965.6567\n",
      "Epoch 306/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72.6240 - val_loss: 1941.8846\n",
      "Epoch 307/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.0074 - val_loss: 1988.8987\n",
      "Epoch 308/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72.7078 - val_loss: 1979.0092\n",
      "Epoch 309/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 76.3671 - val_loss: 2000.5183\n",
      "Epoch 310/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.8746 - val_loss: 1987.7009\n",
      "Epoch 311/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.7856 - val_loss: 1986.4150\n",
      "Epoch 312/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72.8272 - val_loss: 1975.1447\n",
      "Epoch 313/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.3804 - val_loss: 2030.0676\n",
      "Epoch 314/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 71.2988 - val_loss: 2006.8033\n",
      "Epoch 315/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.4609 - val_loss: 1973.4500\n",
      "Epoch 316/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.7137 - val_loss: 1973.6293\n",
      "Epoch 317/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.5062 - val_loss: 1985.9332\n",
      "Epoch 318/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72.0604 - val_loss: 2000.1984\n",
      "Epoch 319/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.3051 - val_loss: 1985.6858\n",
      "Epoch 320/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.5143 - val_loss: 1977.2500\n",
      "Epoch 321/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.0057 - val_loss: 2013.1221\n",
      "Epoch 322/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.5307 - val_loss: 1971.5647\n",
      "Epoch 323/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.8735 - val_loss: 1951.0657\n",
      "Epoch 324/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.9230 - val_loss: 1973.4860\n",
      "Epoch 325/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 78.1687 - val_loss: 1965.7279\n",
      "Epoch 326/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72.8097 - val_loss: 1992.6752\n",
      "Epoch 327/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.8304 - val_loss: 2007.1936\n",
      "Epoch 328/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 76.9023 - val_loss: 2009.1378\n",
      "Epoch 329/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.5561 - val_loss: 1956.5166\n",
      "Epoch 330/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76.0920 - val_loss: 1977.7937\n",
      "Epoch 331/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.8923 - val_loss: 1974.6736\n",
      "Epoch 332/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.0983 - val_loss: 1995.1385\n",
      "Epoch 333/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.0191 - val_loss: 2029.9274\n",
      "Epoch 334/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 69.3827 - val_loss: 1954.5044\n",
      "Epoch 335/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.6668 - val_loss: 1993.6047\n",
      "Epoch 336/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.3024 - val_loss: 1949.3303\n",
      "Epoch 337/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 68.9843 - val_loss: 1981.4734\n",
      "Epoch 338/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.4773 - val_loss: 1979.2881\n",
      "Epoch 339/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.4454 - val_loss: 2019.7339\n",
      "Epoch 340/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 79.5422 - val_loss: 1982.3285\n",
      "Epoch 341/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.1298 - val_loss: 1974.6876\n",
      "Epoch 342/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 70.1393 - val_loss: 2006.4009\n",
      "Epoch 343/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72.2561 - val_loss: 2008.8119\n",
      "Epoch 344/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.2935 - val_loss: 1927.2360\n",
      "Epoch 345/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.1671 - val_loss: 2003.6088\n",
      "Epoch 346/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.9336 - val_loss: 2023.5330\n",
      "Epoch 347/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.6827 - val_loss: 1974.9600\n",
      "Epoch 348/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 77.4443 - val_loss: 2011.9330\n",
      "Epoch 349/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.9186 - val_loss: 1996.6721\n",
      "Epoch 350/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.5115 - val_loss: 1977.6279\n",
      "Epoch 351/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 80.4662 - val_loss: 1983.7273\n",
      "Epoch 352/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.0210 - val_loss: 1959.6771\n",
      "Epoch 353/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.7615 - val_loss: 1990.0023\n",
      "Epoch 354/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.8464 - val_loss: 1961.2795\n",
      "Epoch 355/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 79.9731 - val_loss: 1972.2830\n",
      "Epoch 356/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.5133 - val_loss: 1978.2256\n",
      "Epoch 357/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72.2523 - val_loss: 1991.7192\n",
      "Epoch 358/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 79.6347 - val_loss: 1991.0623\n",
      "Epoch 359/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.0981 - val_loss: 1969.3967\n",
      "Epoch 360/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.9274 - val_loss: 1947.9471\n",
      "Epoch 361/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 77.9318 - val_loss: 1943.3005\n",
      "Epoch 362/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.6816 - val_loss: 2012.2577\n",
      "Epoch 363/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72.3946 - val_loss: 1965.5583\n",
      "Epoch 364/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 77.9886 - val_loss: 2014.1969\n",
      "Epoch 365/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.0721 - val_loss: 1998.0853\n",
      "Epoch 366/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 78.4890 - val_loss: 1984.1049\n",
      "Epoch 367/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.1095 - val_loss: 1993.9104\n",
      "Epoch 368/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.5500 - val_loss: 1966.9613\n",
      "Epoch 369/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 78.4831 - val_loss: 2023.8088\n",
      "Epoch 370/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.6239 - val_loss: 1966.2500\n",
      "Epoch 371/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.8493 - val_loss: 1970.1538\n",
      "Epoch 372/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.5901 - val_loss: 1965.2574\n",
      "Epoch 373/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.5626 - val_loss: 2007.0715\n",
      "Epoch 374/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.3856 - val_loss: 1990.4534\n",
      "Epoch 375/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 69.0671 - val_loss: 1979.8599\n",
      "Epoch 376/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76.5425 - val_loss: 1963.5183\n",
      "Epoch 377/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.8989 - val_loss: 2018.7800\n",
      "Epoch 378/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.1876 - val_loss: 1983.0986\n",
      "Epoch 379/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 78.2710 - val_loss: 1988.8518\n",
      "Epoch 380/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.5687 - val_loss: 1978.1172\n",
      "Epoch 381/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.0935 - val_loss: 1932.2445\n",
      "Epoch 382/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.4147 - val_loss: 1978.0763\n",
      "Epoch 383/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.1960 - val_loss: 1975.4543\n",
      "Epoch 384/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.2113 - val_loss: 1964.2749\n",
      "Epoch 385/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.4733 - val_loss: 2001.9340\n",
      "Epoch 386/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.6903 - val_loss: 1969.7024\n",
      "Epoch 387/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.2247 - val_loss: 1977.2876\n",
      "Epoch 388/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.8002 - val_loss: 1985.3627\n",
      "Epoch 389/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.6193 - val_loss: 2006.2048\n",
      "Epoch 390/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.8153 - val_loss: 1973.9779\n",
      "Epoch 391/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 71.0089 - val_loss: 2001.6293\n",
      "Epoch 392/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.2046 - val_loss: 2001.9666\n",
      "Epoch 393/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.2742 - val_loss: 1997.7837\n",
      "Epoch 394/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.4220 - val_loss: 1981.0237\n",
      "Epoch 395/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 70.8829 - val_loss: 1991.0000\n",
      "Epoch 396/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.8350 - val_loss: 1993.1976\n",
      "Epoch 397/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72.4743 - val_loss: 2025.5603\n",
      "Epoch 398/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.0344 - val_loss: 2001.9584\n",
      "Epoch 399/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.5188 - val_loss: 2003.1392\n",
      "Epoch 400/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.1210 - val_loss: 1959.0760\n",
      "Epoch 401/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.5507 - val_loss: 2000.7594\n",
      "Epoch 402/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 70.9994 - val_loss: 2021.0808\n",
      "Epoch 403/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.8881 - val_loss: 2015.3539\n",
      "Epoch 404/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72.3245 - val_loss: 2016.9279\n",
      "Epoch 405/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 78.6068 - val_loss: 1989.7358\n",
      "Epoch 406/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.6462 - val_loss: 1990.0050\n",
      "Epoch 407/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72.0370 - val_loss: 1974.8337\n",
      "Epoch 408/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.2289 - val_loss: 1994.0881\n",
      "Epoch 409/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.5236 - val_loss: 1999.5123\n",
      "Epoch 410/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.3763 - val_loss: 2035.9379\n",
      "Epoch 411/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.0099 - val_loss: 2000.1416\n",
      "Epoch 412/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 71.9444 - val_loss: 1989.7903\n",
      "Epoch 413/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72.3222 - val_loss: 2018.1970\n",
      "Epoch 414/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72.5131 - val_loss: 1980.8639\n",
      "Epoch 415/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.1668 - val_loss: 1997.8334\n",
      "Epoch 416/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.2007 - val_loss: 1994.4235\n",
      "Epoch 417/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.2738 - val_loss: 1983.1986\n",
      "Epoch 418/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 69.2728 - val_loss: 1937.1788\n",
      "Epoch 419/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.4307 - val_loss: 1975.7640\n",
      "Epoch 420/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 77.1853 - val_loss: 1964.6289\n",
      "Epoch 421/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.1553 - val_loss: 2005.0461\n",
      "Epoch 422/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 79.0554 - val_loss: 2023.0312\n",
      "Epoch 423/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.7446 - val_loss: 1999.9794\n",
      "Epoch 424/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 77.4557 - val_loss: 1958.5502\n",
      "Epoch 425/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.2170 - val_loss: 2002.7882\n",
      "Epoch 426/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.4312 - val_loss: 1981.0806\n",
      "Epoch 427/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.3194 - val_loss: 1980.6770\n",
      "Epoch 428/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.7911 - val_loss: 1989.3812\n",
      "Epoch 429/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.4421 - val_loss: 1977.5918\n",
      "Epoch 430/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.8924 - val_loss: 1981.6680\n",
      "Epoch 431/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 70.6323 - val_loss: 1946.6162\n",
      "Epoch 432/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.1103 - val_loss: 1982.1200\n",
      "Epoch 433/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.5780 - val_loss: 1986.5825\n",
      "Epoch 434/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.6204 - val_loss: 1979.5675\n",
      "Epoch 435/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 70.5697 - val_loss: 1971.5961\n",
      "Epoch 436/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 70.8367 - val_loss: 1982.2528\n",
      "Epoch 437/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.1541 - val_loss: 1951.2275\n",
      "Epoch 438/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.4021 - val_loss: 1953.9034\n",
      "Epoch 439/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 76.9748 - val_loss: 1996.7501\n",
      "Epoch 440/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72.6057 - val_loss: 1983.1541\n",
      "Epoch 441/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72.9884 - val_loss: 1918.0269\n",
      "Epoch 442/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.3610 - val_loss: 1955.3651\n",
      "Epoch 443/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 70.1902 - val_loss: 1969.5980\n",
      "Epoch 444/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.1736 - val_loss: 1975.2246\n",
      "Epoch 445/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 70.1724 - val_loss: 1977.9403\n",
      "Epoch 446/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 67.7096 - val_loss: 1967.7827\n",
      "Epoch 447/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.7974 - val_loss: 1948.0339\n",
      "Epoch 448/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 80.4772 - val_loss: 1942.1602\n",
      "Epoch 449/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 78.1990 - val_loss: 2003.5229\n",
      "Epoch 450/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.7133 - val_loss: 1984.8330\n",
      "Epoch 451/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.8392 - val_loss: 1959.3423\n",
      "Epoch 452/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72.8680 - val_loss: 1995.3602\n",
      "Epoch 453/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.0563 - val_loss: 2007.8041\n",
      "Epoch 454/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 77.1478 - val_loss: 1940.7382\n",
      "Epoch 455/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.9264 - val_loss: 1984.6482\n",
      "Epoch 456/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 67.9856 - val_loss: 1996.0918\n",
      "Epoch 457/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 71.8623 - val_loss: 1972.9702\n",
      "Epoch 458/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 77.4642 - val_loss: 1982.4214\n",
      "Epoch 459/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.6652 - val_loss: 2040.7053\n",
      "Epoch 460/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.7475 - val_loss: 1996.1984\n",
      "Epoch 461/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.9387 - val_loss: 2004.7041\n",
      "Epoch 462/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.9780 - val_loss: 1953.6697\n",
      "Epoch 463/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.2295 - val_loss: 1980.3141\n",
      "Epoch 464/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 74.8302 - val_loss: 1961.8673\n",
      "Epoch 465/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.9715 - val_loss: 1961.3746\n",
      "Epoch 466/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.9353 - val_loss: 1996.9525\n",
      "Epoch 467/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.1873 - val_loss: 1969.0663\n",
      "Epoch 468/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 70.3208 - val_loss: 1992.7931\n",
      "Epoch 469/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 76.4886 - val_loss: 2008.7708\n",
      "Epoch 470/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 80.3025 - val_loss: 1981.5745\n",
      "Epoch 471/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.0546 - val_loss: 1964.1171\n",
      "Epoch 472/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 73.0864 - val_loss: 1952.7314\n",
      "Epoch 473/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72.6847 - val_loss: 2026.2234\n",
      "Epoch 474/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.5414 - val_loss: 2011.3470\n",
      "Epoch 475/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 76.5574 - val_loss: 1945.4232\n",
      "Epoch 476/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.3214 - val_loss: 1951.2029\n",
      "Epoch 477/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 78.3032 - val_loss: 1948.5520\n",
      "Epoch 478/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.4898 - val_loss: 2009.0070\n",
      "Epoch 479/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 74.6485 - val_loss: 2030.9449\n",
      "Epoch 480/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 70.5731 - val_loss: 2012.0729\n",
      "Epoch 481/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 69.7317 - val_loss: 2027.1921\n",
      "Epoch 482/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 71.9451 - val_loss: 2040.6013\n",
      "Epoch 483/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 76.0931 - val_loss: 1996.9386\n",
      "Epoch 484/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 70.6050 - val_loss: 1994.7253\n",
      "Epoch 485/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 72.5087 - val_loss: 2015.0255\n",
      "Epoch 486/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72.2872 - val_loss: 2003.6937\n",
      "Epoch 487/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 76.9577 - val_loss: 1959.9636\n",
      "Epoch 488/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.3782 - val_loss: 2015.0099\n",
      "Epoch 489/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.9215 - val_loss: 1974.8141\n",
      "Epoch 490/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.5146 - val_loss: 2001.4396\n",
      "Epoch 491/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 71.8576 - val_loss: 1987.1543\n",
      "Epoch 492/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.5949 - val_loss: 2036.2118\n",
      "Epoch 493/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 72.6946 - val_loss: 1986.6564\n",
      "Epoch 494/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 75.6657 - val_loss: 2010.1224\n",
      "Epoch 495/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.9394 - val_loss: 1999.7588\n",
      "Epoch 496/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 79.4083 - val_loss: 2018.3335\n",
      "Epoch 497/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 73.3142 - val_loss: 2023.8235\n",
      "Epoch 498/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 75.6664 - val_loss: 1981.3815\n",
      "Epoch 499/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - loss: 71.4371 - val_loss: 1994.5668\n",
      "Epoch 500/500\n",
      "\u001b[1m216/216\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 76.6334 - val_loss: 2028.7260\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x236c32020c0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, epochs=500, batch_size=32, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a59d575f-7ebb-494d-8875-8f9c760e2a71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 867us/step - loss: 1954.2229\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1824.2884521484375"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "038860e5-46fc-4c68-a2ab-f6845bb37e7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 884us/step\n",
      "Value at Risk (95%): [ -40.26604462  -21.14908886  -25.3300152    -3.55839503 -171.57649536]\n",
      "Conditional Value at Risk (95%): GOOG_Close    -63.112988\n",
      "JPM_Close     -26.224147\n",
      "MSFT_Close    -33.209486\n",
      "NVDA_Close     -9.481850\n",
      "VOO_Close    -210.451077\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(X_test)\n",
    "returns = pd.DataFrame(predictions - y_test, columns=closing_prices.columns)  # Calculate returns from predictions\n",
    "\n",
    "def calculate_var(returns, confidence_level=0.95):\n",
    "    sorted_returns = np.sort(returns, axis=0)\n",
    "    index = int((1 - confidence_level) * sorted_returns.shape[0])\n",
    "    return sorted_returns[index]\n",
    "\n",
    "def calculate_cvar(returns, var):\n",
    "    return np.mean(returns[returns <= var], axis=0)\n",
    "\n",
    "var_95 = calculate_var(returns, 0.95)\n",
    "cvar_95 = calculate_cvar(returns, var_95)\n",
    "\n",
    "print(f\"Value at Risk (95%): {var_95}\")\n",
    "print(f\"Conditional Value at Risk (95%): {cvar_95}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f90cb1a-4bbd-4a50-85da-278f0b8beb8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
